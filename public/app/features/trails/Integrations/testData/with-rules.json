{
  "status": "success",
  "data": {
    "groups": [
      {
        "name": "grafana-slo-app-reconcile-latency",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "slo_plugin:slo_reconciliation:latency_seconds_p50",
            "query": "quantile_over_time(0.5,{namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} | logfmt | ( logger=\"plugin.grafana-slo-app\" , componentName=\"slo-controller\" ) | duration_seconds!=\"\" | unwrap duration_seconds[5m]) by (cluster,slug)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:10.192301917Z",
            "evaluationTime": 2.113165797
          },
          {
            "name": "slo_plugin:slo_reconciliation:latency_seconds_p95",
            "query": "quantile_over_time(0.95,{namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} | logfmt | ( logger=\"plugin.grafana-slo-app\" , componentName=\"slo-controller\" ) | duration_seconds!=\"\" | unwrap duration_seconds[5m]) by (cluster,slug)",
            "labels": {
              "app": "grafana-slo-app",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:12.305483642Z",
            "evaluationTime": 2.216145269
          },
          {
            "name": "slo_plugin:slo_reconciliation:latency_seconds_max",
            "query": "max_over_time({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} | logfmt | ( logger=\"plugin.grafana-slo-app\" , componentName=\"slo-controller\" ) | duration_seconds!=\"\" | unwrap duration_seconds[5m]) by (cluster,slug)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:14.521638881Z",
            "evaluationTime": 2.183355367
          },
          {
            "name": "slo_plugin:slo_reconciliation:total_rate_5m",
            "query": "(sum by (cluster,slug)(rate({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} | logfmt | ( logger=\"plugin.grafana-slo-app\" , componentName=\"slo-controller\" ) | duration_seconds!=\"\"[5m])) + sum by (cluster,slug)(rate({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} | logfmt | ( logger=\"plugin.grafana-slo-app\" , componentName=\"slo-controller\" ) | level=\"error\" | msg=\"Failed to reconcile SLO\"[5m])))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:16.705004924Z",
            "evaluationTime": 3.130777895
          },
          {
            "name": "slo_plugin:slo_reconciliation:error_rate_5m",
            "query": "sum(rate({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} | logfmt | ( logger=\"plugin.grafana-slo-app\" , componentName=\"slo-controller\" ) | level=\"error\" | msg=\"Failed to reconcile SLO\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:19.835792105Z",
            "evaluationTime": 3.296517148
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:10.192282145Z",
        "evaluationTime": 12.940033389
      },
      {
        "name": "LogVolumeHistMetrics",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_feature:log_vol:log_query_bytes:rate1m",
            "query": "(sum by (org_id,cluster,namespace)((sum by (org_id,cluster,namespace)(sum_over_time({job=~\".*/(querier|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | source!=\"logvolhist\" | query_type=~\"(filter|limited)\" | unwrap bytes(total_bytes) | __error__=\"\"[1m])) and sum by (org_id,cluster,namespace)(sum_over_time({job=~\".*/(querier|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | source=\"logvolhist\" | unwrap bytes(total_bytes) | __error__=\"\"[1m])))) / 60)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:50.373791119Z",
            "evaluationTime": 1.191668826
          },
          {
            "name": "loki_feature:log_vol:histogram_query_bytes:rate1m",
            "query": "(sum by (org_id,cluster,namespace)(sum_over_time({job=~\".*/(querier|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | source=\"logvolhist\" | unwrap bytes(total_bytes) | __error__=\"\"[1m])) / 60)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:51.565469122Z",
            "evaluationTime": 1.6589903929999998
          },
          {
            "name": "loki_feature:log_vol:log_query_seconds:rate1m",
            "query": "(sum by (org_id,cluster,namespace)((sum by (org_id,cluster,namespace)(sum_over_time({job=~\".*/(querier|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | source!=\"logvolhist\" | query_type=~\"(filter|limited)\" | unwrap duration(duration) | __error__=\"\"[1m])) and sum by (org_id,cluster,namespace)(sum_over_time({job=~\".*/(querier|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | source=\"logvolhist\" | unwrap duration(duration) | __error__=\"\"[1m])))) / 60)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:53.224468284Z",
            "evaluationTime": 1.5034286890000002
          },
          {
            "name": "loki_feature:log_vol:histogram_query_seconds:rate1m",
            "query": "(sum by (org_id,cluster,namespace)(sum_over_time({job=~\".*/(querier|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | source=\"logvolhist\" | unwrap duration(duration) | __error__=\"\"[1m])) / 60)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:54.727905508Z",
            "evaluationTime": 1.5187716679999999
          },
          {
            "name": "loki_feature:log_vol:timeout:rate1m",
            "query": "sum by (org_id,cluster,namespace)(count_over_time({job=~\".*/(query-frontend|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | ( source=\"logvolhist\" , status==499 )[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:56.246690045Z",
            "evaluationTime": 1.3908438300000001
          },
          {
            "name": "loki_feature:log_vol:total:rate1m",
            "query": "sum by (org_id,cluster,namespace)(count_over_time({job=~\".*/(query-frontend|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | source=\"logvolhist\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:57.637542269Z",
            "evaluationTime": 1.434819755
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:50.37376943Z",
        "evaluationTime": 8.698597511
      },
      {
        "name": "amixr",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "AmixrCreateAlertmanagerAlertsSlow",
            "query": "((sum by (task,cluster,namespace)(rate({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/amixr-engine-celery-critical\"} |= \"create_alertmanager_alerts\" |= \"succeeded in\" | pattern \"\u003c_\u003e Task \u003ctask\u003e[\u003cuid\u003e] succeeded in \u003cduration\u003es: \u003c_\u003e\" | duration\u003e20[10m])) * 60) \u003e 30)",
            "annotations": {
              "message": "Too many create_alertmanager_alerts tasks are slow",
              "runbook_url": "https://github.com/grafana/amixr-private/blob/dev/RUNBOOK.md#AmixrCreateAlertmanagerAlertsSlow"
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:28.558275558Z",
            "evaluationTime": 1.6361397389999999
          },
          {
            "state": "inactive",
            "name": "AmixrPersonalNotificationsErrors",
            "query": "((sum by (cluster,namespace)(max by (cluster,namespace,alert_group_id)(max_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/amixr-engine-celery-long\"} |= \"uncompleted personal notifications\" | pattern \"\u003c_\u003eAlert group \u003calert_group_id\u003e has (\u003cmissing\u003e) uncompleted personal notifications\" | unwrap missing | __error__=\"\"[15m]))) / max by (cluster,namespace)(max_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/amixr-engine-celery-long\"} |= \"personal_notifications_triggered\" | pattern \"\u003c_\u003epersonal_notifications_triggered=\u003ctriggered\u003e personal_notifications_completed=\u003ccompleted\u003e\" | unwrap triggered | __error__=\"\"[15m]))) \u003e 0.005)",
            "annotations": {
              "message": "Too many alert group personal notifications are not completing",
              "runbook_url": "https://github.com/grafana/amixr-private/blob/dev/RUNBOOK.md#AmixrPersonalNotificationsErrors"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:30.19442436Z",
            "evaluationTime": 1.865373855
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:28.558235471Z",
        "evaluationTime": 3.5015673830000003
      },
      {
        "name": "ExploreLogsMetrics",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_feature:explore_logs:log_query_bytes:rate1m",
            "query": "sum by (org_id,cluster,namespace,container)(rate({job=~\".*/(.*query-frontend|.*querier.*|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"source=grafana-lokiexplore-app\" | logfmt | source!=\"grafana-lokiexplore-app\" | query_type=~\"(filter|limited)\" | unwrap bytes(total_bytes) | __error__=\"\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:06.904456628Z",
            "evaluationTime": 2.275563667
          },
          {
            "name": "loki_feature:explore_logs:explore_log_query_bytes:rate1m",
            "query": "sum by (org_id,cluster,namespace,container)(rate({job=~\".*/(.*query-frontend|.*querier.*|enterprise-logs-read|loki-read)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" |= \"source=grafana-lokiexplore-app\" | logfmt | source=\"grafana-lokiexplore-app\" | unwrap bytes(total_bytes) | __error__=\"\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:09.180031465Z",
            "evaluationTime": 1.3863552129999999
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:06.904442009Z",
        "evaluationTime": 3.661958195
      },
      {
        "name": "TenantMetricsQueryBytes",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_tenant:query_bytes:rate1m",
            "query": "(sum by (cluster,namespace,org_id,container)(sum_over_time({job=~\".*/(.*query-frontend|.*querier.*|enterprise-logs-read|loki-read|ruler)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | unwrap bytes(total_bytes) | __error__=\"\"[1m])) / 60)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:01:53.213713719Z",
            "evaluationTime": 1.2881839529999999
          },
          {
            "name": "loki_tenant:query_bytes:rate5m",
            "query": "(sum by (cluster,namespace,org_id,container)(sum_over_time({job=~\".*/(.*query-frontend|.*querier.*|enterprise-logs-read|loki-read|ruler)\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" | logfmt | unwrap bytes(total_bytes) | __error__=\"\"[5m])) / 300)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:01:54.501909968Z",
            "evaluationTime": 3.919955603
          }
        ],
        "totals": null,
        "interval": 240,
        "lastEvaluation": "2024-10-12T18:01:53.213690377Z",
        "evaluationTime": 5.208183901
      },
      {
        "name": "k6-backend-prod",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "k6Cloudk6StatsIPLookupFailed",
            "query": "(sum by (cluster,namespace)(rate({namespace=\"k6-stats\", name=\"k6-stats-service\", container=\"http\"} | logfmt | context=\"ip_lookup\"[5m])) \u003e 10)",
            "duration": 300,
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6Cloudk6StatsIPLookupFailed\u0026view=grouped",
              "summary": "IP lookup in k6-stats service is failing too often."
            },
            "labels": {
              "area": "k6-backend",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:30.884092801Z",
            "evaluationTime": 1.109152821
          },
          {
            "state": "inactive",
            "name": "k6CloudTestRunAbortedBySystem",
            "query": "(sum by (cluster,namespace,test_run_id)(count_over_time({namespace=\"k6-cloud\", name=~\"services-api|services-api-consumer|django-celery\", container=~\"http|worker|consumer\"} |= \"event=\\\"Test run status transition\\\"\" | logfmt | run_status_new=\"Aborted (by system)\"[5m])) \u003e 0)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudTestRunAbortedBySystem\u0026view=grouped",
              "description": "Admin Page: https://admin-prod-us-east-0.grafana.net/directory-k6-cloud/admin/k6/loadtestrun/view/{{ $labels.test_run_id }}/\n",
              "runbook_url_internal": "https://github.com/grafana/k6-internal-docs/blob/master/docs/backend/runbooks/alerts_test_run.md#k6cloudtestrunabortedbysystem",
              "summary": "Test Run {{ $labels.test_run_id }} was Aborted by System."
            },
            "labels": {
              "area": "k6-backend",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:31.993257135Z",
            "evaluationTime": 1.8017767679999999
          },
          {
            "state": "inactive",
            "name": "k6CloudTestRunAbortedBySystemMultiple",
            "query": "(count by (cluster,namespace)(sum by (organization_id,cluster,namespace)(count_over_time({namespace=\"k6-cloud\", name=~\"services-api|services-api-consumer|django-celery\", container=~\"http|worker|consumer\"} |= \"event=\\\"Test run status transition\\\"\" | logfmt | run_status_new=\"Aborted (by system)\"[5m]))) \u003e= 3)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudTestRunAbortedBySystemMultiple\u0026view=grouped",
              "description": "Admin Page: https://admin-prod-us-east-0.grafana.net/directory-k6-cloud/admin/k6/loadtestrun/?run_status__exact=6\n",
              "runbook_url_internal": "https://github.com/grafana/k6-internal-docs/blob/master/docs/backend/runbooks/alerts_test_run.md#k6cloudtestrunabortedbysystemmultiple",
              "summary": "Multiple Test Runs were Aborted by System"
            },
            "labels": {
              "area": "k6-backend",
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:33.79504307Z",
            "evaluationTime": 1.9770853499999999
          },
          {
            "state": "inactive",
            "name": "k6CloudIngestTestRunTimedOutMultiple",
            "query": "(count by (cluster,namespace)(sum by (organization_id,cluster,namespace)(count_over_time({namespace=\"k6-cloud\", name=~\"services-api|services-api-consumer|django-celery\", container=~\"http|worker|consumer\"} |= \"event=\\\"Test run status transition\\\"\" | logfmt | run_status_new=\"Timed out\" | run_process=~\".*to Ingest.*\"[5m]))) \u003e 3)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudIngestTestRunTimedOutMultiple\u0026view=grouped",
              "description": "Admin Page: https://admin-prod-us-east-0.grafana.net/directory-k6-cloud/admin/k6/loadtestrun/?load_test__creation_process__exact=K6-INGEST\u0026run_status__exact=4\n",
              "summary": "Multiple Ingest Test Runs timed out"
            },
            "labels": {
              "area": "k6-backend",
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:35.772139579Z",
            "evaluationTime": 1.070462583
          },
          {
            "state": "inactive",
            "name": "k6CloudCloudTestRunTimedOut",
            "query": "(sum by (cluster,namespace,test_run_id)(count_over_time({namespace=\"k6-cloud\", name=~\"services-api|services-api-consumer|django-celery\", container=~\"http|worker|consumer\"} |= \"event=\\\"Test run status transition\\\"\" | logfmt | run_status_new=\"Timed out\" | run_process=~\".*to Cloud.*\"[5m])) \u003e 0)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudCloudTestRunTimedOut\u0026view=grouped",
              "description": "Admin Page: https://admin-prod-us-east-0.grafana.net/directory-k6-cloud/admin/k6/loadtestrun/view/{{ $labels.test_run_id }}/\n",
              "runbook_url_internal": "https://github.com/grafana/k6-internal-docs/blob/master/docs/backend/runbooks/alerts_test_run.md#k6cloudcloudtestruntimedout",
              "summary": "Cloud Test Run {{ $labels.test_run_id }} timed out"
            },
            "labels": {
              "area": "k6-backend",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:36.842609909Z",
            "evaluationTime": 1.156960008
          },
          {
            "state": "inactive",
            "name": "k6CloudCloudTestRunTimedOutMultiple",
            "query": "(count by (cluster,namespace)(sum by (organization_id,cluster,namespace)(count_over_time({namespace=\"k6-cloud\", name=~\"services-api|services-api-consumer|django-celery\", container=~\"http|worker|consumer\"} |= \"event=\\\"Test run status transition\\\"\" | logfmt | run_status_new=\"Timed out\" | run_process=~\".*to Cloud.*\"[5m]))) \u003e 3)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudCloudTestRunTimedOutMultiple\u0026view=grouped",
              "description": "Admin Page: https://admin-prod-us-east-0.grafana.net/directory-k6-cloud/admin/k6/loadtestrun/?load_test__creation_process__exact=K6-CLOUD\u0026run_status__exact=4\n",
              "runbook_url_internal": "https://github.com/grafana/k6-internal-docs/blob/master/docs/backend/runbooks/alerts_test_run.md#k6cloudcloudtestruntimedoutmultiple",
              "summary": "Multiple Cloud Test Runs timed out"
            },
            "labels": {
              "area": "k6-backend",
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:37.999579171Z",
            "evaluationTime": 0.303488329
          },
          {
            "state": "inactive",
            "name": "k6CloudNginxUnableToConnectToUpstream",
            "query": "(sum by (pod,cluster,namespace)(count_over_time({namespace=\"k6-cloud\", container=\"nginx\"} |~ \"no live upstreams while connecting to upstream\"[7m])) \u003e 120)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudNginxUnableToConnectToUpstream\u0026view=grouped",
              "message": "Nginx container in pod {{ $labels.pod }} can not connect to the http upstream",
              "runbook_url_internal": "https://github.com/grafana/k6-internal-docs/blob/master/docs/backend/runbooks/alerts_nginx.md#k6cloudnginxunabletoconnecttoupstream",
              "summary": "Nginx log shows excessive connection errors connecting to http upstream"
            },
            "labels": {
              "area": "k6-backend",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:38.303078984Z",
            "evaluationTime": 0.408345443
          },
          {
            "state": "inactive",
            "name": "k6CloudNginxTooMany500Errors",
            "query": "(sum by (cluster,namespace,name)(count_over_time({namespace=\"k6-cloud\", container=\"nginx\"} |~ \"HTTP/1.\\\\d\\\" 500\"[5m])) \u003e 50)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudNginxTooMany500Errors\u0026view=grouped",
              "message": "Nginx container in pod {{ $labels.pod }} has too many 500 errors for 5 mins",
              "runbook_url_internal": "https://github.com/grafana/k6-internal-docs/blob/master/docs/backend/runbooks/alerts_nginx.md#k6cloudnginxunabletoconnecttoupstream",
              "summary": "Nginx for service {{ $labels.name }} has too many 500 errors"
            },
            "labels": {
              "area": "k6-backend",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:38.711435542Z",
            "evaluationTime": 0.633382352
          },
          {
            "state": "inactive",
            "name": "k6CloudNginxTooMany50xErrors",
            "query": "(sum by (cluster,namespace,pod)(count_over_time({namespace=\"k6-cloud\", container=\"nginx\"} |~ \"HTTP/1.\\\\d\\\" 50[1-9]\"[5m])) \u003e 50)",
            "annotations": {
              "alert_url": "https://ops.grafana-ops.net/alerting/list?search=label:area%3Dk6-backend%20namespace:%20{{ if (match \"dev-.+\" $labels.cluster) }}dev{{ else }}ops{{ end }}%20k6CloudNginxTooMany50xErrors\u0026view=grouped",
              "message": "Nginx container in pod {{ $labels.pod }} has too many 50x errors for 5 mins",
              "runbook_url_internal": "https://github.com/grafana/k6-internal-docs/blob/master/docs/backend/runbooks/alerts_nginx.md#k6cloudnginxtoomany50xerrors",
              "summary": "Nginx pod {{ $labels.pod }} has too many 50x errors"
            },
            "labels": {
              "area": "k6-backend",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:39.344827305Z",
            "evaluationTime": 2.010658613
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:30.884066426Z",
        "evaluationTime": 10.471424717
      },
      {
        "name": "k6-insights",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "k6InsightsHighErrorsLogged",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"k6-cloud-insights-(staging|prod|ops)\", app=\"k6-cloud-insights\"} | json | level=~\"error|panic|fatal\"[30s])) \u003e 0)",
            "duration": 300,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/jasdlfahOFHoas/k6-insights-logs?var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}\u0026var-container={{$labels.container}}{{ if eq $labels.cluster \"dev-us-east-0\" }}\u0026var-metrics_data_source=dev-cortex{{ end }}",
              "summary": "Errors logged by Insights service: {{ $labels.cluster }} {{ $labels.namespace }} {{ $labels.container }}."
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:35.934158125Z",
            "evaluationTime": 0.582465801
          },
          {
            "state": "inactive",
            "name": "k6InsightsCriticalErrorsLogged",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"k6-cloud-insights-(staging|prod|ops)\", app=\"k6-cloud-insights\"} | json | level=~\"error|panic|fatal\"[30s])) \u003e 0)",
            "duration": 600,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/jasdlfahOFHoas/k6-insights-logs?var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}\u0026var-container={{$labels.container}}{{ if eq $labels.cluster \"dev-us-east-0\" }}\u0026var-metrics_data_source=dev-cortex{{ end }}",
              "summary": "errors Errors logged by Insights service for long time: {{ $labels.cluster }} {{ $labels.namespace }} {{ $labels.container }}."
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:36.516632584Z",
            "evaluationTime": 0.578543393
          },
          {
            "state": "inactive",
            "name": "k6InsightsWarningsLogged",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"k6-cloud-insights-(staging|prod|ops)\", app=\"k6-cloud-insights\"} | json | level=\"warn\"[30s])) \u003e 0)",
            "duration": 300,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/jasdlfahOFHoas/k6-insights-logs?var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}\u0026var-container={{$labels.container}}{{ if eq $labels.cluster \"dev-us-east-0\" }}\u0026var-metrics_data_source=dev-cortex{{ end }}",
              "summary": "Warnings logged by Insights service: {{ $labels.cluster }} {{ $labels.namespace }} {{ $labels.container }}."
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:37.095187896Z",
            "evaluationTime": 1.831628397
          },
          {
            "state": "inactive",
            "name": "TimescaledbHighErrorsLogged",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"k6-cloud-insights-(staging|prod|ops)\", app=\"timescaledb\"} |= \"error\"[30s])) \u003e 0)",
            "duration": 30,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/jasdlfahOFHoas/k6-insights-logs?var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}\u0026var-container={{$labels.container}}{{ if eq $labels.cluster \"dev-us-east-0\" }}\u0026var-metrics_data_source=dev-cortex{{ end }}",
              "summary": "Errors logged by Timescaledb."
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:38.926827088Z",
            "evaluationTime": 0.846338206
          },
          {
            "state": "inactive",
            "name": "TimescaledbCriticalErrorsLogged",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"k6-cloud-insights-(staging|prod|ops)\", app=\"timescaledb\"} |= \"error\"[30s])) \u003e 0)",
            "duration": 600,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/jasdlfahOFHoas/k6-insights-logs?var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}\u0026var-container={{$labels.container}}{{ if eq $labels.cluster \"dev-us-east-0\" }}\u0026var-metrics_data_source=dev-cortex{{ end }}",
              "summary": "Errors logged by Timescaledb for long time."
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:39.773172277Z",
            "evaluationTime": 0.842083651
          },
          {
            "state": "inactive",
            "name": "k6InsightsTempoOverridesAllLevelsLogged",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"tempo.+\", job=~\"tempo-.+/query-frontend\"} | logfmt | component=\"overrides-api\" |~ \"k6-cloud-insights\"[30s])) \u003e 0)",
            "duration": 30,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22c9i%22%3A%7B%22datasource%22%3A%22OP27Xzxnk%22%2C%22queries%22%3A%5B%7B%22refId%22%3A%22A%22%2C%22expr%22%3A%22%7Bnamespace%3D%7E%5C%22tempo.%2B%5C%22%2C+job%3D%7E%5C%22tempo-.%2B%2Fquery-frontend%5C%22%7D+%7C+logfmt+%7C+component%3D%5C%22overrides-api%5C%22+%7C+level%21%3D%5C%22info%5C%22+%7C%3D+%5C%22k6-cloud-insights%5C%22%22%2C%22queryType%22%3A%22range%22%2C%22datasource%22%3A%7B%22type%22%3A%22loki%22%2C%22uid%22%3A%22OP27Xzxnk%22%7D%2C%22editorMode%22%3A%22code%22%7D%5D%2C%22range%22%3A%7B%22from%22%3A%22now-1h%22%2C%22to%22%3A%22now%22%7D%7D%7D\u0026orgId=1",
              "summary": "k6 x Tempo overrides message logged: {{ $labels.cluster }} {{ $labels.namespace }} {{ $labels.container }}."
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:40.615266366Z",
            "evaluationTime": 0.635772352
          },
          {
            "state": "inactive",
            "name": "k6InsightsInvalidClientServerProtocol",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"k6-cloud-insights-(staging|prod|ops)\", app=\"k6-cloud-insights\"} |= \"server gave HTTP response to HTTPS client\"[1m])) \u003e 0)",
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/jasdlfahOFHoas/k6-insights-logs?var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}\u0026var-container={{$labels.container}}{{ if eq $labels.cluster \"dev-us-east-0\" }}\u0026var-metrics_data_source=dev-cortex{{ end }}",
              "summary": "Server gave HTTP response to HTTPS client: {{ $labels.cluster }} {{ $labels.namespace }} {{ $labels.container }}."
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:41.251049265Z",
            "evaluationTime": 1.876090852
          },
          {
            "state": "inactive",
            "name": "k6InsightsTooFrequentTimescaledbCheckpoints",
            "query": "(sum by (cluster,namespace,container)(count_over_time({namespace=~\"k6-cloud-insights-(staging|prod|ops)\", app=\"timescaledb\"} |= \"checkpoints are occurring too frequently\"[30s])) \u003e 0)",
            "duration": 300,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/242343234/k6-insights-timescale-overview?var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}{{ if eq $labels.cluster \"dev-us-east-0\" }}\u0026var-metrics_data_source=dev-cortex{{ end }}\u0026viewPanel=panel-75",
              "summary": "Timescaledb checkpoints are happening too frequently. The WAL is reaching its maximum size more often than it should, which can potentially lead to running out of disk space in its volume: {{ $labels.cluster }} {{ $labels.namespace }} {{ $labels.container }}."
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:43.127150618Z",
            "evaluationTime": 2.042566272
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:35.934130568Z",
        "evaluationTime": 9.235592101
      },
      {
        "name": "AWS Logs",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "AgentWALWriteFailures",
            "query": "(sum by (cluster,namespace,pod)(count_over_time({job=~\"aws-logs.+/grafana-agent\"} |= \"failed to write entry\"[5m])) \u003e 0)",
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/aws-logs-push/aws-logs-push?orgId=1\u0026refresh=1m\u0026from=now-1h\u0026to=now\u0026var-cluster={{ $labels.cluster }}\u0026var-namespace={{ $labels.namespace }}",
              "runbook_url": "https://github.com/grafana/cloud-onboarding/blob/main/runbooks/AWSLogs.md#agent-wal-write-failures",
              "summary": "Agent {{$labels.pod}} in cluster.namespace {{$labels.cluster}}.{{$labels.namespace}} has failed to write entries to the WAL"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:25.291896248Z",
            "evaluationTime": 1.5351918850000001
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:25.291867191Z",
        "evaluationTime": 1.5352256199999998
      },
      {
        "name": "tempo_cloud_alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "TempoMetricsGeneratorCannotRemoteWriteForTenant",
            "query": "sum by (cluster,namespace,metrics_cluster,user,msg)(count_over_time({container=\"metrics-instance-mapper\", namespace=~\"tracing.*|tempo.*\"} |~ \"(invalid metrics target provided|status code not 2xx while proxying)\" !~ \"(err-mimir-max-series-per-(metric|user)|err-mimir-tenant-max-ingestion-rate|err-mimir-tenant-max-request-rate|err-mimir-max-label-names-per-series|metrics instance does not belong to this cluster)\" | logfmt[1m]))",
            "duration": 600,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/Ia2Xj-u7k/f27bff83-f08b-5091-ab74-347499bde846?orgId=1\u0026refresh=1m\u0026var-datasource=000000134\u0026var-cluster={{ $labels.cluster }}\u0026var-namespace={{ $labels.namespace }}\u0026var-tenant={{ $labels.user }}",
              "message": "The tenant {{ $labels.user }} cannot remote-write samples in {{ $labels.cluster }}/{{ $labels.namespace }} to cluster {{ $labels.metrics_cluster }}.\n",
              "reason": "{{ $labels.msg }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/tempo/playbooks.md#TempoMetricsGeneratorCannotRemoteWriteForTenant"
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:45.067380064Z",
            "evaluationTime": 0.422555966
          },
          {
            "state": "inactive",
            "name": "TempoMetricsGeneratorInstanceDoesNotBelongToThisCluster",
            "query": "sum by (cluster,namespace,metrics_cluster,user,msg)(count_over_time({container=\"metrics-instance-mapper\", namespace=~\"tracing.*|tempo.*\"} |~ \"(metrics instance does not belong to this cluster)\" | logfmt[1m]))",
            "duration": 600,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/Ia2Xj-u7k/f27bff83-f08b-5091-ab74-347499bde846?orgId=1\u0026refresh=1m\u0026var-datasource=000000134\u0026var-cluster={{ $labels.cluster }}\u0026var-namespace={{ $labels.namespace }}\u0026var-tenant={{ $labels.user }}",
              "message": "The tenant {{ $labels.user }} cannot remote-write samples in {{ $labels.cluster }}/{{ $labels.namespace }} to cluster {{ $labels.metrics_cluster }} because they are in different regions.\n",
              "reason": "{{ $labels.msg }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/tempo/playbooks.md#TempoMetricsGeneratorInstanceDoesNotBelongToThisCluster"
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:45.489946808Z",
            "evaluationTime": 1.8122641160000001
          },
          {
            "state": "inactive",
            "name": "TempoUserConfigurableOverridesReloadingFailed",
            "query": "(sum by (cluster,namespace)(count_over_time({namespace=~\"tracing.*|tempo.*\"} | logfmt | msg=\"failed to refresh user-configurable config\"[5m])) \u003e 5)",
            "duration": 300,
            "annotations": {
              "message": "Reloading user-configurable overrides is failing in {{ $labels.cluster }}/{{ $labels.namespace }}.\n",
              "reason": "{{ $labels.msg }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/tempo/playbooks.md#TempoUserConfigurableOverridesReloadingFailed"
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:47.302219917Z",
            "evaluationTime": 2.911654178
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:45.067355839Z",
        "evaluationTime": 5.146524386
      },
      {
        "name": "updater_alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "UpdaterIsFailingInDrone",
            "query": "(sum by (attrs_io_drone_repo_slug,attrs_io_drone_build_number)(count_over_time({host=~\"drone-.*\", job!=\"syslog\"} | json | __error__=\"\" | attrs_tag=~`us.gcr.io/kubernetes-dev/drone/plugins/(updater|deploy-image|update-jsonnet-attribute).+` | line_format \"{{.log}}\" | logfmt | __error__=\"\" | level=\"fatal\" |= \"failed to push some refs\"[1h])) \u003e= 1)",
            "annotations": {
              "message": "The updater has failed to push refs in the following Drone build: https://drone.grafana.net/{{ $labels.attrs_io_drone_repo_slug }}/{{ $labels.attrs_io_drone_build_number }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/runbooks/updater.md"
            },
            "labels": {
              "severity": "warning",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:54.478655014Z",
            "evaluationTime": 1.109427214
          },
          {
            "state": "inactive",
            "name": "UpdaterIsFailingInArgoWorkflows",
            "query": "(sum by (job)(count_over_time({container=\"main\", template_name=\"git-updater\"} | logfmt | __error__=\"\" | level=\"fatal\" |= \"failed to push some refs\"[1h])) \u003e= 1)",
            "annotations": {
              "message": "The updater has failed to push refs in the following Argo Workflow: https://argo-workflows.grafana.net/workflows/{{ $labels.job }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/runbooks/updater.md"
            },
            "labels": {
              "severity": "warning",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:55.588094031Z",
            "evaluationTime": 0.498609887
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:54.478631664Z",
        "evaluationTime": 1.608076868
      },
      {
        "name": "synthetic_monitoring_loki_rules",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "synthetic_monitoring:cluster_namespace_name:panic:count5m",
            "query": "sum by (cluster,namespace,name)(count_over_time({namespace=~\"synthetic-monitoring.*|sm-proxy\"} |~ \"^panic: \"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:57.870785162Z",
            "evaluationTime": 1.957856063
          },
          {
            "name": "synthetic_monitoring:cluster_namespace:api_remote_validation_unhandled_error:rate5m",
            "query": "sum by (cluster,namespace)(rate({name=\"synthetic-monitoring-api\", namespace=~\"synthetic-monitoring.*|sm-proxy\"} |= \"unhandled error while validating remotes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:59.828653211Z",
            "evaluationTime": 1.077928949
          },
          {
            "name": "synthetic_monitoring:cluster_namespace_org_error:api_disabled_tenant:rate5m",
            "query": "count by (cluster,namespace,orgId,stackId,tenantId,plan,reason,error)(count by (cluster,namespace,orgId,stackId,tenantId,plan,reason,error)(rate({name=\"synthetic-monitoring-api\", namespace=~\"synthetic-monitoring.*|sm-proxy\"} |= \"disabling tenant\" | json | _error_=\"\"[5m])))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:00.906589601Z",
            "evaluationTime": 1.592967313
          },
          {
            "name": "synthetic_monitoring:reason_error:api_disabled_tenant_count:rate1m",
            "query": "count by (reason,error)(count by (orgId,stackId,reason,error)(rate({name=\"synthetic-monitoring-api\", namespace=~\"synthetic-monitoring.*|sm-proxy\"} |= \"disabling tenant\" | json | _error_=\"\"[1m])))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:02.499567669Z",
            "evaluationTime": 1.529722037
          },
          {
            "name": "synthetic_monitoring:agent_invalid_scope_errors:rate1m",
            "query": "sum by (region,tenant)(rate({namespace=\"sm-proxy\", container=\"agent\"} | json | __error__=\"\" | error=~`.* HTTP status 401 .*\\\"error\\\":\\\"authentication error: invalid scope requested\\\".*`[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:04.029299215Z",
            "evaluationTime": 0.660063024
          },
          {
            "name": "synthetic_monitoring:cluster_namespace:api_missing_plan_error:rate5m",
            "query": "count by (cluster,namespace)(rate({name=\"synthetic-monitoring-api\", namespace=~\"synthetic-monitoring.*|sm-proxy\"} |= \"translating plan name\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:04.689373234Z",
            "evaluationTime": 0.989248894
          },
          {
            "name": "synthetic_monitoring:region_tenant_cause_status_code:store_stream_failed:rate1m",
            "query": "sum by (region,tenant,cause,status_code)(label_replace(rate({cluster=~\"pop-.*\", namespace=\"sm-proxy\", container=\"agent\"} |= \"\\\"message\\\":\\\"store stream failed\\\"\" | json | __error__=\"\" | error=~\".*authentication error:.*\"[1m]),\"cause\",\"$1\",\"error\",\".*\\\"error\\\":\\\"([^\\\"]+)\\\".*\"))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:05.678635031Z",
            "evaluationTime": 0.438713951
          },
          {
            "name": "synthetic_monitoring:cluster_node:seccomp_errors:rate15m",
            "query": "sum by (cluster,node)(label_replace(rate({cluster=~\"pop-.*\"} |= \"error loading seccomp filter into kernel: error loading seccomp filter\" | json | __error__=\"\"[15m]),\"node\",\"$1\",\"event_source_host\",\"(.+)\"))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:06.117358887Z",
            "evaluationTime": 1.460155072
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:57.870771785Z",
        "evaluationTime": 9.706747854
      },
      {
        "name": "HostedGrafanaEdge-prod-eu-west-0",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:http_requests:rate5m",
            "query": "sum by (cluster,slug,response_code,response_flags,exclude_from_sla)(rate({name=\"hosted-grafana-gateway\", container=\"istio-proxy\", cluster=\"prod-eu-west-0\"} |= \"-grafana-http.hosted-grafana.svc.cluster.local\" | regexp \"(?P\u003cslug\u003e\\\\w+)-grafana-http.hosted-grafana.svc.cluster.local\" | json | label_format exclude_from_sla=\"{{if eq (regexReplaceAll \\\"/apis/.*|/api/live/ws|/api/datasources/proxy/.*|/api/ds/query|/api/tsdb/query|/api/datasources/.+/health|/api/datasources/.+/resources.*|/api/datasources/uid/.+/resources/*|/api/plugins/.+/resources.*|/api/plugin-proxy/.*|/api/.+/rules|/api/prometheus/.+/api/v1/rules|/api/ruler/.+/api/v1/rules\\\" .path \\\"exclude_from_sla\\\")  \\\"exclude_from_sla\\\"}}true{{else}}false{{end}}\"[5m]))",
            "labels": {
              "namespace": "hosted-grafana"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:00:54.388331561Z",
            "evaluationTime": 2.206720615
          }
        ],
        "totals": null,
        "interval": 240,
        "lastEvaluation": "2024-10-12T18:00:54.388309245Z",
        "evaluationTime": 2.206752339
      },
      {
        "name": "grafana-slo-operator-reconcile-latency",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "slo_operator:slo_reconciliation:latency_seconds_p50",
            "query": "quantile_over_time(0.5,{namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\", cluster=~\".+\"} | json | componentName=\"slo-controller\" | duration_seconds!=\"\" | unwrap duration_seconds[5m]) by (cluster,namespace_extracted)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:23.474652819Z",
            "evaluationTime": 0.486552911
          },
          {
            "name": "slo_operator:slo_reconciliation:latency_seconds_p95",
            "query": "quantile_over_time(0.95,{namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\", cluster=~\".+\"} | json | componentName=\"slo-controller\" | duration_seconds!=\"\" | unwrap duration_seconds[5m]) by (cluster,namespace_extracted)",
            "labels": {
              "app": "grafana-slo-operator",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:23.961216354Z",
            "evaluationTime": 1.9175791260000001
          },
          {
            "name": "slo_operator:slo_reconciliation:latency_seconds_max",
            "query": "max_over_time({namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\", cluster=~\".+\"} | json | componentName=\"slo-controller\" | duration_seconds!=\"\" | unwrap duration_seconds[5m]) by (cluster,namespace_extracted)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:25.878806832Z",
            "evaluationTime": 0.478451569
          },
          {
            "name": "slo_operator:slo_reconciliation:total_rate_5m",
            "query": "sum by (cluster,namespace_extracted)(rate({namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\", cluster=~\".+\"} | json | componentName=\"slo-controller\" | _message=~\"Failed to reconcile SLO|Successfully reconciled SLO\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:26.357269946Z",
            "evaluationTime": 1.527375615
          },
          {
            "name": "slo_operator:slo_reconciliation:error_rate_5m",
            "query": "sum by (cluster,namespace_extracted)(rate({namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\", cluster=~\".+\"} | json | componentName=\"slo-controller\" | _level=\"error\" | _message=\"Failed to reconcile SLO\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:27.884657077Z",
            "evaluationTime": 0.539161959
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:23.474633196Z",
        "evaluationTime": 4.9491919620000004
      },
      {
        "name": "drone",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "DroneConversionFailure",
            "query": "(sum(count_over_time({job=\"drone/drone-server\"} | json | repo=\"grafana/deployment_tools\" | ref=\"refs/heads/master\" | msg=\"trigger: cannot convert yaml\"[1m])) \u003e 0)",
            "annotations": {
              "message": "The Drone conversion extension failed for a commit to deployment_tools master",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/drone/README.md#DroneConversionFailure"
            },
            "labels": {
              "severity": "warning",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:26.289367785Z",
            "evaluationTime": 1.6049355840000001
          },
          {
            "state": "inactive",
            "name": "DroneVaultErrors",
            "query": "(sum(count_over_time({name=\"drone-vault\"} | logfmt | level=\"error\"[1m])) \u003e 0)",
            "annotations": {
              "message": "The Drone Vault proxy has encountered an error. Check the logs for more information and make sure Drone builds are still working.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/drone/README.md#DroneVaultErrors"
            },
            "labels": {
              "severity": "warning",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:27.894314728Z",
            "evaluationTime": 0.803985813
          },
          {
            "state": "inactive",
            "name": "KubeManifestsExporterRunningForTooLong",
            "query": "max(((max_over_time({job=\"drone/drone-exporter\"} | logfmt | msg=\"Build listed\" | repo=\"grafana/deployment_tools\" | ( runningStages=~`.*,?kube-manifests-exporter - apply,?.*` or pendingStages=~`.*,?kube-manifests-exporter - apply,?.*` ) | unwrap duration_seconds(buildTimeSinceStarted)[5m]) \u003e 600) \u003c 1e+06))",
            "annotations": {
              "message": "The `kube-manifests-exporter` stage has been running for more than 10 minutes. Check the latest deployments_tools master Drone build to make sure it's not stuck.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/drone/README.md#KubeManifestsExporterRunningForTooLong"
            },
            "labels": {
              "severity": "warning",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:28.69830762Z",
            "evaluationTime": 0.502248696
          },
          {
            "state": "inactive",
            "name": "KubeManifestsExporterRunningForTooLong",
            "query": "max(((max_over_time({job=\"drone/drone-exporter\"} | logfmt | msg=\"Build listed\" | repo=\"grafana/deployment_tools\" | ( runningStages=~`.*,?kube-manifests-exporter - apply,?.*` or pendingStages=~`.*,?kube-manifests-exporter - apply,?.*` ) | unwrap duration_seconds(buildTimeSinceStarted)[5m]) \u003e 1200) \u003c 1e+06))",
            "annotations": {
              "message": "The `kube-manifests-exporter` stage has been running for more than 20 minutes. Check the latest deployments_tools master Drone build to make sure it's not stuck.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/drone/README.md#KubeManifestsExporterRunningForTooLong"
            },
            "labels": {
              "severity": "critical",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:29.200566567Z",
            "evaluationTime": 0.362458094
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:26.28934352Z",
        "evaluationTime": 3.273685956
      },
      {
        "name": "loki_cell_usage_insights",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-009\", cluster=\"dev-eu-west-2\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:56.872060646Z",
            "evaluationTime": 0.946495902
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-boltdb-shipper\", cluster=\"dev-us-central-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:57.818567248Z",
            "evaluationTime": 1.634675567
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-005\", cluster=\"dev-us-central-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:59.453252718Z",
            "evaluationTime": 0.822920736
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-006\", cluster=\"dev-us-central-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:00.276181689Z",
            "evaluationTime": 1.466171964
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-010\", cluster=\"dev-us-central-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:01.742361994Z",
            "evaluationTime": 1.483722133
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-001\", cluster=\"dev-us-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:03.226092892Z",
            "evaluationTime": 0.831796601
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-002\", cluster=\"dev-us-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:04.057899639Z",
            "evaluationTime": 1.609990347
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-012\", cluster=\"dev-us-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:05.667899951Z",
            "evaluationTime": 0.7578476
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-013\", cluster=\"dev-us-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:06.425758568Z",
            "evaluationTime": 1.550605453
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-dev-014\", cluster=\"dev-us-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:07.196841628Z",
            "evaluationTime": 1.834537496
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-ops-002\", cluster=\"ops-eu-south-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:09.031389864Z",
            "evaluationTime": 0.873784668
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-030\", cluster=\"prod-ap-northeast-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:09.905184819Z",
            "evaluationTime": 1.853225048
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-014\", cluster=\"prod-ap-south-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:11.758420063Z",
            "evaluationTime": 2.077083943
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-028\", cluster=\"prod-ap-south-1\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:13.835514563Z",
            "evaluationTime": 0.586472262
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-011\", cluster=\"prod-ap-southeast-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:14.421997973Z",
            "evaluationTime": 1.801371627
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-020\", cluster=\"prod-ap-southeast-1\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:16.223386353Z",
            "evaluationTime": 1.6404496750000002
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-032\", cluster=\"prod-ap-southeast-2\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:17.863844261Z",
            "evaluationTime": 1.042640661
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-004\", cluster=\"prod-au-southeast-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:18.906494182Z",
            "evaluationTime": 0.807602565
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-026\", cluster=\"prod-au-southeast-1\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:19.714110519Z",
            "evaluationTime": 0.962620419
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-018\", cluster=\"prod-ca-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:20.676740933Z",
            "evaluationTime": 0.771623836
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-025\", cluster=\"prod-eu-north-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:21.448373858Z",
            "evaluationTime": 0.683380333
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod\", cluster=\"prod-eu-west-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:22.131765683Z",
            "evaluationTime": 1.4000128840000001
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-005\", cluster=\"prod-eu-west-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:23.531788285Z",
            "evaluationTime": 0.887905761
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-019\", cluster=\"prod-eu-west-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:24.419704381Z",
            "evaluationTime": 0.735054156
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-022\", cluster=\"prod-eu-west-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:25.154768626Z",
            "evaluationTime": 1.649685283
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-012\", cluster=\"prod-eu-west-2\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:26.804463475Z",
            "evaluationTime": 1.870878583
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-029\", cluster=\"prod-eu-west-2\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:28.675350565Z",
            "evaluationTime": 0.439522959
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-031\", cluster=\"prod-eu-west-2\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:29.114883185Z",
            "evaluationTime": 0.301253166
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-013\", cluster=\"prod-eu-west-3\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:29.416150174Z",
            "evaluationTime": 1.3707423
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-008\", cluster=\"prod-gb-south-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:30.786905999Z",
            "evaluationTime": 1.673847446
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-034\", cluster=\"prod-me-central-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:32.46075998Z",
            "evaluationTime": 0.434839462
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-033\", cluster=\"prod-me-central-1\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:32.895609631Z",
            "evaluationTime": 1.8339326
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-015\", cluster=\"prod-sa-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:34.729552112Z",
            "evaluationTime": 1.3545726710000001
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-023\", cluster=\"prod-sa-east-1\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:36.084133712Z",
            "evaluationTime": 0.188516075
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-024\", cluster=\"prod-sa-east-1\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:36.272660262Z",
            "evaluationTime": 0.097376371
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod\", cluster=\"prod-us-central-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:36.370046187Z",
            "evaluationTime": 1.11094465
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod2\", cluster=\"prod-us-central-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:37.480999186Z",
            "evaluationTime": 0.492998832
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-017\", cluster=\"prod-us-central-5\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:37.974009065Z",
            "evaluationTime": 1.8214214850000001
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod3\", cluster=\"prod-us-central-5\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:39.795440261Z",
            "evaluationTime": 1.5320963650000001
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-006\", cluster=\"prod-us-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:41.3275489Z",
            "evaluationTime": 0.839804244
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-010\", cluster=\"prod-us-east-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:42.167362126Z",
            "evaluationTime": 0.351390783
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod-021\", cluster=\"prod-us-west-0\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:42.51876166Z",
            "evaluationTime": 1.810170237
          },
          {
            "name": "loki_cell:usage_insights:total_bytes_absent_over_time:sum5m",
            "query": "sum by (cluster,namespace)(absent_over_time({job=~\".*/cortex-gw\", namespace=\"loki-prod\", cluster=\"us-central2\"} |= \"request timings\" |= \"total_bytes\"[5m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:44.328941198Z",
            "evaluationTime": 0.037695075
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:01:56.871660379Z",
        "evaluationTime": 47.494980101
      },
      {
        "name": "unified_alerting",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "UnifiedAlertingAlertmanagerConfigFailure",
            "query": "(sum by (cluster,namespace,slug)(count_over_time({job=\"hosted-grafana/grafana\"} |= \"ngalert\" |~ \"(?i)Alertmanager\" | logfmt | ( ( ( ( logger=\"ngalert.notifier.multiorg-alertmanager\" , level=\"error\" ) , msg=\"Failed to apply Alertmanager configuration\" ) or ( ( logger=\"ngalert.multiorg.alertmanager\" , ( lvl=\"eror\" or level=\"error\" ) ) , msg=\"failed to apply Alertmanager config for org\" ) ) or ( logger=\"alertmanager\" , msg=\"unable to sync configuration\" ) )[5m])) \u003e 0)",
            "duration": 900,
            "annotations": {
              "message": "instance {{ $labels.slug }} on cluster {{ $labels.cluster }} is failing to apply its Alertmanager config and therefore is not sending any Grafana managed alerts.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/hosted-grafana/runbooks.md#UnifiedAlertingAlertmanagerConfigFailure"
            },
            "labels": {
              "severity": "critical",
              "team": "alerting"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:55.163219733Z",
            "evaluationTime": 3.029903567
          },
          {
            "state": "inactive",
            "name": "UnifiedAlertingLokiRangeToInstantFailure",
            "query": "(sum by (cluster,namespace,slug)(rate({job=\"hosted-grafana/grafana\", container=\"grafana\"} |= \"Could not migrate rule from range to instant query\"[1m])) \u003e 0)",
            "duration": 900,
            "annotations": {
              "message": "instance {{ $labels.slug }} on cluster {{ $labels.cluster }} in namespace {{ $labels.namespace }} is failing to convert a range query to an instant query.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/hosted-grafana/runbooks.md#UnifiedAlertingLokiRangeToInstantFailure"
            },
            "labels": {
              "severity": "warning",
              "team": "alerting"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:58.193133285Z",
            "evaluationTime": 1.734317291
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:55.163195578Z",
        "evaluationTime": 4.764259589
      },
      {
        "name": "agent_metrics_parquet_file_size",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "AgentMetricsParquetFileSize",
            "query": "(((sum by (namespace,cluster,job,table)(sum_over_time({namespace=\"usage-service\", job=\"usage-service/analytics\"} |= \"loaded parquet file\" | json | unwrap size[1h])) / 1024) / 1024) \u003e 10)",
            "annotations": {
              "message": "The agent metrics parquet file size generated on cluster {{ $labels.cluster }} has exceeded {{ $labels.table }} MiB\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/insights/usage_service_runbook.md#AgentMetricsParquetFileSize"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:40.67908966Z",
            "evaluationTime": 0.886679205
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:40.679070342Z",
        "evaluationTime": 0.88670366
      },
      {
        "name": "loki-freetier-limiter",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "LokiFreetierLimiterUserNotifiedMoreThanOnce",
            "query": "(count by (cluster,namespace,user_id)(count_over_time({container=\"freetier-limiter\", namespace=~\"loki.*\"} | logfmt | msg=\"Notifying limited user\"[30m])) \u003e 1)",
            "duration": 900,
            "annotations": {
              "message": "User {{ $labels.user }} limited has been notified more than once.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/loki/playbooks.md#LokiFreetierLimiterUserNotifiedMoreThanOnce"
            },
            "labels": {
              "ownership": "ingest-squad",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:15.890902752Z",
            "evaluationTime": 0.63982897
          },
          {
            "state": "inactive",
            "name": "LokiFreetierLimiterUserNotNotified",
            "query": "(count by (cluster,namespace,user_id)(count_over_time({container=\"freetier-limiter\", namespace=~\"loki.*\"} | logfmt | msg=\"Failed to notify user about being limited\"[7m])) \u003e 0)",
            "annotations": {
              "message": "Failed to notify user {{ $labels.user }} about being limited.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/loki/playbooks.md#LokiFreetierLimiterUserNotNotified"
            },
            "labels": {
              "ownership": "ingest-squad",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:16.530740433Z",
            "evaluationTime": 1.827171377
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:15.890880864Z",
        "evaluationTime": 2.467036393
      },
      {
        "name": "KarpenterUnconsolidatable",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "UnconsolidatableNotAllPodsWouldSchedule",
            "query": "sum by (cluster)(count_over_time({job=\"infra-monitoring/eventrouter\", namespace=\"default\"} | logfmt | component=\"karpenter\" | reason=\"Unconsolidatable\" | message=~\"not all pods would schedule,.*\" | message!~\".*would schedule against uninitialized nodeclaim.*\"[30m]))",
            "duration": 86400,
            "annotations": {
              "logs_url": "https://admin-ops-eu-south-0.grafana-ops.net/grafana/explore?schemaVersion=1\u0026panes=%7B%22vam%22:%7B%22datasource%22:%22loki-ops%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bjob%3D%5C%22infra-monitoring%2Feventrouter%5C%22,%20namespace%3D%5C%22default%5C%22%7D%5Cn%20%20%20%20%7C%20logfmt%20%5Cn%20%20%20%20%7C%20component%3D%5C%22karpenter%5C%22%20%5Cn%20%20%20%20%7C%20reason%3D%5C%22Unconsolidatable%5C%22%20%5Cn%20%20%20%20%7C%20message%3D~%5C%22not%20all%20pods%20would%20schedule,.%2A%5C%22%20%5Cn%20%20%20%20%7C%20message%21~%5C%22.%2Awould%20schedule%20against%20uninitialized%20nodeclaim.%2A%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22loki-ops%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-30m%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "message": "Karpenter is failing to consolidate the cluster {{ $labels.cluster }} because there's at least one pod for wich it can't find a suitable node to schedule it in after consolidation. Logs will show the error Karpenter is outputting.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/karpenter.md#UnconsolidatableNotAllPodsWouldSchedule"
            },
            "labels": {
              "severity": "warning",
              "team": "capacity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:27.918634952Z",
            "evaluationTime": 1.090987616
          },
          {
            "state": "inactive",
            "name": "ErrorRateHigh",
            "query": "(sum by (cluster)(count_over_time({namespace=\"karpenter\", job=\"karpenter/karpenter\"} !~ \"(?i)could not schedule pod\" !~ \"(?i)UnfulfillableCapacity\" !~ \"(?i)InsufficientInstanceCapacity\" | json | level=\"ERROR\"[1m])) \u003e 5)",
            "duration": 300,
            "annotations": {
              "logs_url": "https://admin-ops-eu-south-0.grafana-ops.net/grafana/explore?schemaVersion=1\u0026panes=%7B%22sbk%22:%7B%22datasource%22:%22loki-ops%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22sum%20by%20%28cluster%29%20%28%5Cn%20%20%20%20count_over_time%28%5Cn%20%20%20%20%7Bnamespace%3D%5C%22karpenter%5C%22,%20job%3D%5C%22karpenter%2Fkarpenter%5C%22%7D%5Cn%20%20%20%20%21~%20%60%28%3Fi%29could%20not%20schedule%20pod%60%5Cn%20%20%20%20%21~%20%60%28%3Fi%29UnfulfillableCapacity%60%5Cn%20%20%20%20%21~%20%60%28%3Fi%29InsufficientInstanceCapacity%60%5Cn%20%20%20%20%7C%20json%5Cn%20%20%20%20%7C%20level%20%3D%20%60ERROR%60%5Cn%20%20%20%20%5B1m%5D%5Cn%20%20%20%20%29%5Cn%29%20%3E%205%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22loki-ops%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-30m%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "message": "Karpenter is experiencing high log error rates in the {{ $labels.cluster }} cluster, excluding UnfulfillableCapacity and InsufficientInstanceCapacity errors. Please investigate.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/karpenter.md#ErrorRateHigh"
            },
            "labels": {
              "severity": "warning",
              "team": "capacity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:29.009634673Z",
            "evaluationTime": 1.8316256389999999
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:27.918610456Z",
        "evaluationTime": 2.92265562
      },
      {
        "name": "loki_alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "LokiRulerCannotRemoteWriteForTenant",
            "query": "sum by (cluster,namespace,metrics_cluster,user,msg)(count_over_time({container=\"metrics-instance-mapper\", namespace=~\"loki.*\"} |~ \"(invalid metrics target provided|metrics instance does not belong to this cluster)\" | logfmt[1m]))",
            "duration": 300,
            "annotations": {
              "message": "The tenant {{ $labels.user }} cannot remote-write samples in {{ $labels.cluster }}/{{ $labels.namespace }} to cluster {{ $labels.metrics_cluster }}.\n",
              "reason": "{{ $labels.msg }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/loki/playbooks.md#LokiRulerCannotRemoteWriteForTenant"
            },
            "labels": {
              "ownership": "ingest-squad",
              "severity": "notify"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:17.403307755Z",
            "evaluationTime": 2.017144039
          },
          {
            "state": "inactive",
            "name": "CloudDataArchiverPanics",
            "query": "(sum by (cluster,namespace,job)(count_over_time({namespace=~\"loki.*\", container=\"cloud-logs-archiver\"} |= \"panic\"[10m])) \u003e 0)",
            "annotations": {
              "message": "{{ $labels.job }} is experiencing {{ printf \"%.2f\" $value }}% increase of panics in the last 10m.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/loki/playbooks.md#CloudDataArchiverPanics"
            },
            "labels": {
              "ownership": "ingest-squad",
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:19.420460872Z",
            "evaluationTime": 1.5985516849999999
          },
          {
            "state": "inactive",
            "name": "LokiAutoscalerMissingMetric",
            "query": "count by (cluster,scaledObject_Namespace,scaledObject_Name,scaler,error)(count_over_time({namespace=\"keda\"} |= \"error getting metric for scaler\" | pattern \"\u003c_\u003e{\u003cjson_content\u003e}\" | line_format \"{ {{.json_content}} }\" | json | scaledObject_Namespace=~\"loki.*\"[1m]))",
            "duration": 300,
            "annotations": {
              "message": "The KEDA scaler \"{{ $labels.scaler }}\" in the {{ $labels.scaledObject_Name }} ScaledObject from {{ $labels.cluster }}/{{ $labels.scaledObject_Namespace }} is failing to evaluate the scaling metric.\n",
              "reason": "{{ $labels.error }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/loki/playbooks.md#LokiAutoscalerMissingMetric"
            },
            "labels": {
              "ownership": "query-squad",
              "severity": "critical",
              "team": "loki"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:21.019023546Z",
            "evaluationTime": 2.007767812
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:17.403276989Z",
        "evaluationTime": 5.623519861
      },
      {
        "name": "HostedGrafanaDataSourcePluginLogs2m",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:grafana_datasource_loki_orgs_stacks_queries:count2m",
            "query": "sum by (org,stackId)(count_over_time({app=\"grafana\", namespace=\"hosted-grafana\"} |= \"Plugin Request Completed\" | logfmt | __error__=\"\" | logger=\"plugin.instrumentation\" | endpoint=\"queryData\" | pluginId=\"loki\" | dsUID=~\"grafanacloud.*\"[2m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:15.989600348Z",
            "evaluationTime": 3.119072361
          },
          {
            "name": "hosted_grafana:grafana_datasource_mimir_orgs_stacks_queries:count2m",
            "query": "sum by (org,stackId)(count_over_time({app=\"grafana\", namespace=\"hosted-grafana\"} |= \"Plugin Request Completed\" | logfmt | __error__=\"\" | logger=\"plugin.instrumentation\" | endpoint=\"queryData\" | pluginId=\"prometheus\" | dsUID=~\"grafanacloud.*\"[2m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:19.108685155Z",
            "evaluationTime": 3.146386822
          },
          {
            "name": "hosted_grafana:grafana_datasource_tempo_orgs_stacks_queries:count2m",
            "query": "sum by (org,stackId)(count_over_time({app=\"grafana\", namespace=\"hosted-grafana\"} |= \"Plugin Request Completed\" | logfmt | __error__=\"\" | logger=\"plugin.instrumentation\" | endpoint=\"queryData\" | pluginId=\"tempo\" | dsUID=~\"grafanacloud.*\"[2m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:22.255083507Z",
            "evaluationTime": 2.892767612
          },
          {
            "name": "hosted_grafana:grafana_datasource_pyroscope_orgs_stacks_queries:count2m",
            "query": "sum by (org,stackId)(count_over_time({app=\"grafana\", namespace=\"hosted-grafana\"} |= \"Plugin Request Completed\" | logfmt | __error__=\"\" | logger=\"plugin.instrumentation\" | endpoint=\"queryData\" | pluginId=\"grafana-pyroscope-datasource\" | dsUID=~\"grafanacloud.*\"[2m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:25.147860679Z",
            "evaluationTime": 2.81605473
          },
          {
            "name": "hosted_grafana:grafana_explore_logs_requests:count2m",
            "query": "sum by (slug,method,handler,api,status)(count_over_time({namespace=\"hosted-grafana\"} |= \"grafana-lokiexplore-app\" |= \"logger=context\" | logfmt | referer=~`.+grafana-lokiexplore-app.+` | pattern \"\u003c_\u003e/api/datasources/uid/grafanacloud-logs/resources\u003capi\u003e \u003c_\u003e\"[2m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:27.963926567Z",
            "evaluationTime": 2.3706036
          }
        ],
        "totals": null,
        "interval": 120,
        "lastEvaluation": "2024-10-12T18:02:15.989569463Z",
        "evaluationTime": 14.344967013
      },
      {
        "name": "gms-loki-migration-stats",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "GMSResourceMigrationFailed",
            "query": "(sum by (cluster,stackID)(count_over_time({namespace=\"grafana-cloud-migration-service\", name=\"grafana-cloud-migration-service\"} !~ \"(context canceled|grafana api error)\" | logfmt | level=\"error\"[5m])) \u003e 0)",
            "duration": 60,
            "annotations": {
              "logs_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22vzz%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bnamespace%3D%5C%22grafana-cloud-migration-service%5C%22,%20name%3D%5C%22grafana-cloud-migration-service%5C%22%7D%20%21~%20%60%28context%20canceled%7Cgrafana%20api%20error%29%60%20%7C%20logfmt%20%7C%20level%20%3D%20%60error%60%20%7C%20stackID%20%3D%20%60{{ $labels.stackID }}%60%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-30m%22,%22to%22:%22now%22%7D%7D%7D",
              "message": "Stack {{ $labels.stackID }} had errors migrating their resources.\n"
            },
            "labels": {
              "namespace": "grafana-cloud-migration-service",
              "severity": "warning",
              "team": "grafana_enterprise"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:38.245288315Z",
            "evaluationTime": 0.987278666
          },
          {
            "state": "inactive",
            "name": "GMSErrorsReportedFromSource",
            "query": "(sum by (cluster,stackID)(count_over_time({namespace=\"grafana-cloud-migration-service\", name=\"grafana-cloud-migration-service\"} |~ \"received onPremEvent\" | logfmt | error!=\"\"[5m])) \u003e 0)",
            "annotations": {
              "logs_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22vzz%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bnamespace%3D%5C%22grafana-cloud-migration-service%5C%22,%20name%3D%5C%22grafana-cloud-migration-service%5C%22%7D%20%7C~%20%60received%20onPremEvent%60%20%7C%20logfmt%20%7C%20error%20%21%3D%20%60%60%20%7C%20stackID%20%3D%20%60{{ $labels.stackID }}%60%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-15m%22,%22to%22:%22now%22%7D,%22panelsState%22:%7B%22logs%22:%7B%22columns%22:%7B%220%22:%22Time%22,%221%22:%22Line%22,%222%22:%22error%22,%223%22:%22event%22%7D,%22visualisationType%22:%22table%22,%22labelFieldName%22:%22labels%22,%22refId%22:%22A%22%7D%7D%7D%7D",
              "message": "Stack {{ $labels.stackID }} had errors reported from their source instance while migrating.\n"
            },
            "labels": {
              "namespace": "grafana-cloud-migration-service",
              "severity": "warning",
              "team": "grafana_enterprise"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:39.232576038Z",
            "evaluationTime": 0.770471879
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:38.245264463Z",
        "evaluationTime": 1.757788678
      },
      {
        "name": "cloudwatch_exporter_alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "CloudWatchExporterFatalError",
            "query": "(sum by (cluster,namespace,pod)(count_over_time({job=\"hosted-exporters/cloudwatch-exporter-primary\"} |= \"fatal error\"[5m])) \u003e 0)",
            "annotations": {
              "runbook_url": "https://github.com/grafana/cloud-onboarding/blob/main/runbooks/HostedExporters-CloudWatchExporters.md#cloudwatchexporterfatalerror",
              "summary": "The pod {{$labels.pod}} in cluster {{$labels.cluster}} has crashed due to a fatal error"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:11.967237221Z",
            "evaluationTime": 1.175247846
          },
          {
            "state": "inactive",
            "name": "CloudWatchExporterJobsTimingOut",
            "query": "(sum by (namespace,cluster,instance_id,metrics_id)(count_over_time({job=\"hosted-exporters/cloudwatch-exporter-primary\"} |= \"request handled\" | logfmt | ( status_code=\"503\" , duration\u003e2m0s )[20m])) \u003e= 2)",
            "annotations": {
              "runbook_url": "https://github.com/grafana/cloud-onboarding/blob/main/runbooks/HostedExporters-CloudWatchExporters.md#CloudWatchExporterJobsTimingOut",
              "summary": "The CloudWatch exporter job {{$labels.instance_id}}/{{$labels.metrics_id}} has timed out two or more times in the last 20 minutes on {{$labels.cluster}}"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:13.142496766Z",
            "evaluationTime": 1.121182574
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:11.967214797Z",
        "evaluationTime": 2.296469642
      },
      {
        "name": "mimir_health",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "mimir_health:cluster_namespace_pod:crash:count2m",
            "query": "sum by (cluster,namespace,pod)(count_over_time({namespace=~\"(mimir|cortex).*\"} |~ \"^(panic|fatal error): \"[2m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:36.861641311Z",
            "evaluationTime": 1.080667
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:36.861626882Z",
        "evaluationTime": 1.080687374
      },
      {
        "name": "adaptive_metrics_query_stats",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "mimir_user:adaptive_metrics_query_errors:rate1m",
            "query": "sum by (cluster,namespace,user)(rate({job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" |= \"Can't query aggregated metric\" | logfmt[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:29.197947826Z",
            "evaluationTime": 2.030106714
          },
          {
            "name": "mimir_user:adaptive_metrics_api_requests:rate1m",
            "query": "sum by (cluster,namespace,user,attribution)(rate({job=~\".*/(aggregations-api)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"aggregations API call\\\"\" | logfmt | label_format attribution=\"\\n          {{- if hasPrefix \\\"adaptive-metrics/\\\" .User_Agent }}adaptive-metrics\\n          {{- else if hasPrefix \\\"gh-action-autoapply\\\" .User_Agent }}auto-apply\\n          {{- else if hasPrefix \\\"Terraform\\\" .User_Agent }}terraform\\n          {{- else if hasPrefix \\\"distributor-forwarding\\\" .User_Agent }}distributor-forwarding\\n          {{- else if hasPrefix \\\"Grafana\\\" .User_Agent }}grafana-plugin\\n          {{- else if ne \\\"\\\" .X_Grafana_User }}grafana-plugin\\n          {{- else}}api{{end -}}\\n          \" | label_format user=tenant | __error__=\"\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:31.22806373Z",
            "evaluationTime": 1.189412057
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:29.197926546Z",
        "evaluationTime": 3.219555629
      },
      {
        "name": "grafana-labels-app-requests",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "labels_plugin:requests:rate5m",
            "query": "sum by (slug,cluster,status)(rate({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} |~ \"path=/api/plugins/grafana-labels-app/resources.*\" |~ \"status=[1-5][0-9][0-9]\" | pattern \"\u003c_\u003e status=\u003cstatus\u003e \u003c_\u003e\"[5m]))",
            "labels": {
              "app": "grafana-labels-app",
              "team": "irm"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:13.557377893Z",
            "evaluationTime": 2.421986711
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:13.557370294Z",
        "evaluationTime": 2.422000161
      },
      {
        "name": "unused_metrics_file_size_exceeded_limit",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "UnusedMetricsFileSizeExceededLimit",
            "query": "(count_over_time({namespace=\"usage-service\", job=\"usage-service/usage-service\"} |= \"uncompressed file exceeded\"[1h]) \u003e 0)",
            "annotations": {
              "message": "The Unused metrics service on cluster {{ $labels.cluster }} is trying to uncompress a large file and ignoring it\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/insights/usage_service_runbook.md#UnusedMetricsFileSizeExceededLimit"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:11.93607204Z",
            "evaluationTime": 1.187655339
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:11.936052607Z",
        "evaluationTime": 1.18768086
      },
      {
        "name": "GrafanaOnCall",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "grafana_oncall:total_api_requests",
            "query": "sum by (cluster,namespace)(count_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/amixr-engine\"} |= \"inbound\" != \"/health/\" != \"/ready\" != \"path=/ \"[5m]))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:55.769565622Z",
            "evaluationTime": 1.663479745
          },
          {
            "name": "grafana_oncall:failed_api_requests",
            "query": "sum by (cluster,namespace)((count_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/amixr-engine\"} |= \"inbound\" != \"/health/\" != \"/ready\" != \"path=/ \" |~ \"slow=1|status=5\"[5m]) or (count_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/amixr-engine\"} |= \"inbound\" != \"/health/\" != \"/ready\" != \"path=/ \"[5m]) * 0)))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:57.433055986Z",
            "evaluationTime": 2.354468963
          },
          {
            "name": "grafana_oncall:alert_groups_notifications_failed_48h",
            "query": "min by (cluster,namespace)(min_over_time({namespace=~\"amixr-staging|amixr-prod\"} |= \"check_escalation_finished_task\" |= \"Alert groups failing escalation:\" | pattern \"\u003c_\u003eAlert groups failing escalation: \u003ccount\u003e\" | unwrap count | __error__=\"\"[15m]))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:59.787536469Z",
            "evaluationTime": 2.052545427
          },
          {
            "name": "grafana_oncall:alert_groups_notifications_succeeded_48h",
            "query": "min by (cluster,namespace)(min_over_time({namespace=~\"amixr-staging|amixr-prod\"} |= \"check_escalation_finished_task\" |= \"Alert groups succeeding escalation:\" | pattern \"\u003c_\u003eAlert groups succeeding escalation: \u003ccount\u003e\" | unwrap count | __error__=\"\"[15m]))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:01.840094744Z",
            "evaluationTime": 1.12934492
          },
          {
            "name": "grafana_oncall:alert_groups_notifications_total_48h",
            "query": "min by (cluster,namespace)(min_over_time({namespace=~\"amixr-staging|amixr-prod\"} |= \"check_escalation_finished_task\" |= \"Alert groups total escalations:\" | pattern \"\u003c_\u003eAlert groups total escalations: \u003ccount\u003e\" | unwrap count | __error__=\"\"[15m]))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:02.96945069Z",
            "evaluationTime": 2.183439836
          },
          {
            "name": "grafana_oncall:alert_groups_notifications_success_ratio_over_48h",
            "query": "min by (cluster,namespace)(min_over_time({namespace=~\"amixr-staging|amixr-prod\"} |= \"Alert group notifications success ratio:\" | pattern \"\u003c_\u003eAlert group notifications success ratio: \u003cratio\u003e\" | unwrap ratio | __error__=\"\"[15m]))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:05.15290121Z",
            "evaluationTime": 1.615240799
          },
          {
            "name": "grafana_oncall:alert_groups_ingestion_creation_delta_avg_over_48h",
            "query": "min by (cluster,namespace)(min_over_time({namespace=~\"amixr-staging|amixr-prod\"} |= \"Alert group ingestion/creation avg delta seconds:\" | pattern \"\u003c_\u003eAlert group ingestion/creation avg delta seconds: \u003cavg_delta\u003e\" | unwrap avg_delta | __error__=\"\"[15m]))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:06.76815107Z",
            "evaluationTime": 0.97748033
          },
          {
            "name": "grafana_oncall:alert_groups_ingestion_creation_delta_max_over_48h",
            "query": "min by (cluster,namespace)(min_over_time({namespace=~\"amixr-staging|amixr-prod\"} |= \"Alert group ingestion/creation max delta seconds:\" | pattern \"\u003c_\u003eAlert group ingestion/creation max delta seconds: \u003cmax_delta\u003e\" | unwrap max_delta | __error__=\"\"[15m]))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:06.595072298Z",
            "evaluationTime": 2.162425828
          },
          {
            "name": "grafana_oncall:alert_groups_metrics_requests_succeeded",
            "query": "sum by (cluster,namespace,job)((count_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/oncall-engine-exporter(.*)\"} |= \"inbound\" |= \"/metrics\" |= \"slow=0\" |= \"status=2\"[1h]) or (count_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/oncall-engine-exporter(.*)\"} |= \"inbound\" |= \"/metrics\"[1h]) * 0)))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:08.757507906Z",
            "evaluationTime": 1.044416877
          },
          {
            "name": "grafana_oncall:alert_groups_metrics_requests_total",
            "query": "sum by (cluster,namespace,job)((count_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/oncall-engine-exporter(.*)\"} |= \"inbound\" |= \"/metrics\" != \"status=3\"[1h]) or (count_over_time({namespace=~\"amixr-(dev|staging|prod)\", job=~\"amixr-(dev|staging|prod)/oncall-engine-exporter(.*)\"} |= \"inbound\" |= \"/metrics\"[1h]) * 0)))",
            "labels": {
              "namespace": "grafana-oncall"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:09.801935679Z",
            "evaluationTime": 0.239918337
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:01:55.769042667Z",
        "evaluationTime": 14.272816975
      },
      {
        "name": "graphite_index_dump_loki_alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "GraphiteIndexDumpStaleInprog",
            "query": "count_over_time({container=\"gr-index-dumper\"} |= \"STALE INPROG:\"[30m])",
            "duration": 3600,
            "annotations": {
              "message": "Stale .inprog files in index dumping data directory.",
              "runbook_url": "https://github.com/grafana/transmog/blob/master/docs/operations/index_dump.md#stale-inprog-files"
            },
            "labels": {
              "severity": "warning",
              "team": "query"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:07.554445567Z",
            "evaluationTime": 1.820864563
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:07.554421033Z",
        "evaluationTime": 1.820894139
      },
      {
        "name": "CellIngesterQueryThroughput:rate1m",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_cell:ingester_throughput:rate1m",
            "query": "sum by (cluster,namespace,container)(rate({job=~\"loki.*/.*querier\"} |= \"metrics.go\" | logfmt | unwrap bytes(ingester_chunk_decompressed_bytes)[1m]))",
            "labels": {
              "source": "memchunk"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:14.958631538Z",
            "evaluationTime": 0.926899714
          },
          {
            "name": "loki_cell:ingester_throughput:rate1m",
            "query": "sum by (cluster,namespace,container)(rate({job=~\"loki.*/.*querier\"} |= \"metrics.go\" | logfmt | unwrap bytes(ingester_chunk_head_bytes)[1m]))",
            "labels": {
              "source": "head"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:15.885543056Z",
            "evaluationTime": 1.6440660980000001
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:14.958620442Z",
        "evaluationTime": 2.57099506
      },
      {
        "name": "Hosted Exporters API",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "HostedExportersAPIPipelineCreationFailed",
            "query": "(sum by (cluster,namespace,job_type)(count_over_time({job=~\"hosted-exporters/hosted-exporters-api.*\"} | logfmt | msg=~\"pipelines creation failed.*\"[5m])) \u003e 0)",
            "annotations": {
              "dashboard_url": "https://admin-ops-us-east-0.grafana-ops.net/grafana/d/7df07aab1026160304a8c8fb96ad26da/overview-hosted-exporters?orgId=1\u0026refresh=1m\u0026var-datasource=ops-cortex\u0026var-datasourcelogs=loki-ops\u0026var-cluster={{$labels.cluster}}\u0026var-namespace={{$labels.namespace}}",
              "runbook_url": "https://github.com/grafana/cloud-onboarding/blob/main/runbooks/HostedExporters-API.md#hostedexportersapipipelinecreationfailed",
              "summary": "Hosted exporters API can not create a job pipeline for {{$labels.job_type}} for some stacks on {{$labels.cluster}}.{{$labels.namespace}}"
            },
            "labels": {
              "namespace": "hosted-exporters",
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:48.022628643Z",
            "evaluationTime": 0.994400446
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:48.022605268Z",
        "evaluationTime": 0.994430285
      },
      {
        "name": "appplatform.apiserver.rules",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "cluster_slug_app:app_platform_authnz_errors:rate1m",
            "query": "sum by (cluster,slug)(rate({job=\"hosted-grafana/grafana\"} != \"logger=tsdb.loki\" |= \"level=error\" |= \"logger=plugin.grafana-slo-app\" |= \"componentName=slo-controller\" |= \"Unauthorized\"[1m]))",
            "labels": {
              "app": "grafana-slo-app"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:17.82928941Z",
            "evaluationTime": 2.628845655
          },
          {
            "name": "cluster_slug_app:app_platform_missing_auth_errors:rate1m",
            "query": "sum by (cluster,slug)(rate({job=\"hosted-grafana/grafana\"} != \"logger=tsdb.loki\" |= \"level=error\" |= \"logger=plugin.grafana-slo-app\" |= \"kubernetes auth is not configured\"[1m]))",
            "labels": {
              "app": "grafana-slo-app"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:20.458147374Z",
            "evaluationTime": 2.537167036
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:17.829256477Z",
        "evaluationTime": 5.166064909
      },
      {
        "name": "mysql_bigquery_sync_alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "MySQLBigQuerySyncJobNotRun",
            "query": "((max by (job,cluster,namespace,table)(count_over_time({job=~\".*/bigquery-sync-mysql\"} |= \"\\\"status\\\": \\\"succeeded\" | pattern \"\u003c_\u003e\\\"table\\\": \\\"\u003ctable\u003e\\\"\u003c_\u003e\"[1h1m])) or (max by (job,cluster,namespace,table)(count_over_time({job=~\".*/bigquery-sync-mysql\"} |= \"\\\"status\\\": \\\"succeeded\" | pattern \"\u003c_\u003e\\\"table\\\": \\\"\u003ctable\u003e\\\"\u003c_\u003e\"[1h1m] offset 4h0m0s)) * 0)) \u003c 1)",
            "duration": 10800,
            "annotations": {
              "message": "The MySQL -\u003e Bigquery job on cluster {{ $labels.cluster }}, job {{ $labels.job }}, table {{ $labels.table }} has not run for 3 hours!\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/ksonnet/lib/bigquery-sync-mixin/docs/playbook.md#MySQLBigQuerySyncJobNotRun"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:38.029053659Z",
            "evaluationTime": 1.370500904
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:38.029037913Z",
        "evaluationTime": 1.370522058
      },
      {
        "name": "query_summary_failures",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "QuerySummaryFailures",
            "query": "((count_over_time({container=\"recommendations\"} |= \"failed to retrieve query summary\"[1h]) / count_over_time({container=\"recommendations\"} |= \"running complete analysis\"[1h])) \u003e 5)",
            "annotations": {
              "message": "The aggregations recommendations service on cluster {{ $labels.cluster }} in namespace {{ $labels.namespace }} is not getting query summaries from the metrics API\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/insights/usage_service_runbook.md#QuerySummaryFailures"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:16.233294319Z",
            "evaluationTime": 2.215326517
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:16.233251239Z",
        "evaluationTime": 2.215375035
      },
      {
        "name": "grafana-slo-operator-alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "SLOMToperatorControllerErrors",
            "query": "sum by (cluster,namespace_extracted,slug)(count_over_time({namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\"} | json | logger=\"grafana-slo-operator\" | _level=\"error\" | action!=\"\"[10m]))",
            "annotations": {
              "description": "{{ $labels.namespace_extracted }} {{ if $labels.slug }}(slug:{{$labels.slug}}) {{ else }}{{ end }}has errors in controller processes on their instance. This is likely due to a production issue, a bug, or a user hitting a downstream usage limit like mimir rule groups.",
              "query_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22eze%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7Bnamespace%3D%5C%22grafana-slo-operator%5C%22,container%3D%5C%22grafana-slo-operator%5C%22%7D%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7Cjson%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C_level%20%3D%20%5C%22error%5C%22%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7Cnamespace_extracted%3D%20%5C%22{{ $labels.namespace_extracted }}%5C%22%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20action%21%3D%5C%22%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-24h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "summary": "MTO Controller Errors for the {{ $labels.namespace_extracted }} stack"
            },
            "labels": {
              "app": "grafana-slo-operator",
              "severity": "low",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:01:41.651504078Z",
            "evaluationTime": 1.652804169
          },
          {
            "state": "firing",
            "name": "SLOMToperatorDriftCorrectorErrors",
            "query": "sum by (cluster,namespace_extracted,slug)(count_over_time({namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\"} | json | logger=\"grafana-slo-operator\" | _level=\"error\" | componentName=\"drift-corrector\"[1h10m]))",
            "annotations": {
              "description": "{{ $labels.namespace_extracted }} {{ if $labels.slug }}(slug:{{$labels.slug}}) {{ else }}{{ end }}has errors in drift correction processes on their instance. This is likely due to a production issue, a bug, or a user hitting a downstream usage limit like mimir rule groups.",
              "query_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22eze%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bcontainer%3D%5C%22grafana-slo-operator%5C%22,%20debug%3D%5C%22%5C%22,%20namespace%3D%5C%22grafana-slo-operator%5C%22%7D%5Cn%20%20%7C%20json%5Cn%20%20%7C%20logger%3D%5C%22grafana-slo-operator%5C%22%20%7C%20_level%3D%5C%22error%5C%22%20%7C%20componentName%3D%5C%22drift-corrector%5C%22%20%7C%20namespace_extracted%3D%5C%22{{ $labels.namespace_extracted }}%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-24h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "summary": "Drift correction failing for the {{ $labels.namespace_extracted }} stack"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "SLOMToperatorDriftCorrectorErrors",
                  "app": "grafana-slo-operator",
                  "cluster": "prod-eu-west-0",
                  "namespace_extracted": "stacks-542894",
                  "severity": "low",
                  "slug": "aize",
                  "team": "gops_slos"
                },
                "annotations": {
                  "description": "stacks-542894 (slug:aize) has errors in drift correction processes on their instance. This is likely due to a production issue, a bug, or a user hitting a downstream usage limit like mimir rule groups.",
                  "query_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22eze%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bcontainer%3D%5C%22grafana-slo-operator%5C%22,%20debug%3D%5C%22%5C%22,%20namespace%3D%5C%22grafana-slo-operator%5C%22%7D%5Cn%20%20%7C%20json%5Cn%20%20%7C%20logger%3D%5C%22grafana-slo-operator%5C%22%20%7C%20_level%3D%5C%22error%5C%22%20%7C%20componentName%3D%5C%22drift-corrector%5C%22%20%7C%20namespace_extracted%3D%5C%22stacks-542894%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-24h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
                  "summary": "Drift correction failing for the stacks-542894 stack"
                },
                "state": "firing",
                "activeAt": "2024-10-12T13:36:41.650413336Z",
                "value": "1e+00"
              },
              {
                "labels": {
                  "alertname": "SLOMToperatorDriftCorrectorErrors",
                  "app": "grafana-slo-operator",
                  "cluster": "prod-us-east-0",
                  "namespace_extracted": "stacks-770248",
                  "severity": "low",
                  "slug": "aurora",
                  "team": "gops_slos"
                },
                "annotations": {
                  "description": "stacks-770248 (slug:aurora) has errors in drift correction processes on their instance. This is likely due to a production issue, a bug, or a user hitting a downstream usage limit like mimir rule groups.",
                  "query_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22eze%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bcontainer%3D%5C%22grafana-slo-operator%5C%22,%20debug%3D%5C%22%5C%22,%20namespace%3D%5C%22grafana-slo-operator%5C%22%7D%5Cn%20%20%7C%20json%5Cn%20%20%7C%20logger%3D%5C%22grafana-slo-operator%5C%22%20%7C%20_level%3D%5C%22error%5C%22%20%7C%20componentName%3D%5C%22drift-corrector%5C%22%20%7C%20namespace_extracted%3D%5C%22stacks-770248%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-24h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
                  "summary": "Drift correction failing for the stacks-770248 stack"
                },
                "state": "firing",
                "activeAt": "2024-10-12T17:01:41.650413336Z",
                "value": "2e+00"
              }
            ],
            "labels": {
              "app": "grafana-slo-operator",
              "severity": "low",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:01:43.304317477Z",
            "evaluationTime": 0.234179242
          },
          {
            "state": "inactive",
            "name": "SLOMToperator500Errors",
            "query": "(sum by (cluster,namespace_extracted,slug)(count_over_time({namespace=\"grafana-slo-operator\", container=\"grafana-slo-operator\", debug=\"\"} | json | ( logger=\"grafana-slo-operator\" , _level=\"error\" ) |~ \"code=5[0-9][012356789]\"[10m])) \u003e 1)",
            "annotations": {
              "description": "{{ $labels.namespace_extracted }} {{ if $labels.slug }}(slug:{{$labels.slug}}) {{ else }}{{ end }}has 5XX errors on their instance. View the logs in the query_url. This could be caused by a bug or by bad configuration!",
              "query_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22eze%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7Bnamespace%3D%5C%22grafana-slo-operator%5C%22,%20container%3D%5C%22grafana-slo-operator%5C%22,%20debug%3D%5C%22%5C%22%7D%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20json%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20logger%3D%5C%22grafana-slo-operator%5C%22,%20_level%3D%5C%22error%5C%22%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20namespace_extracted%20%3D%20%5C%22{{ $labels.namespace_extracted }}%5C%22%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20We%20ignore%20504s%20because%20they%20are%20transient%20issues%20that%20we%20retry.%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C~%20%5C%22code%3D5%5B0-9%5D%5B012356789%5D%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-24h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "summary": "5XX Errors for the {{ $labels.namespace_extracted }} stack"
            },
            "labels": {
              "app": "grafana-slo-operator",
              "severity": "warn",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:01:43.538505063Z",
            "evaluationTime": 0.745550559
          },
          {
            "state": "inactive",
            "name": "SLOMToperatorSlowReconciliation",
            "query": "sum by (cluster,namespace_extracted,slug)(count_over_time({namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\"} | json | logger=\"grafana-slo-operator\" | componentName=\"slo-controller\" | duration_seconds\u003e45[10m]))",
            "annotations": {
              "description": "{{ $labels.namespace_extracted }} {{ if $labels.slug }}(slug:{{$labels.slug}}) {{ else }}{{ end }}has slow reconciliation times, implying failures to retry, or configuration that is preventing reconciliation of dashboards and rule_groups.",
              "query_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22eze%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7Bnamespace%3D%5C%22grafana-slo-operator%5C%22,%20debug%3D%5C%22%5C%22,%20container%3D%5C%22grafana-slo-operator%5C%22%7D%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20json%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20logger%3D%5C%22grafana-slo-operator%5C%22%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20componentName%3D%5C%22slo-controller%5C%22%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20namespace_extracted%3D%5C%22{{ $labels.namespace_extracted }}%5C%22%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7C%20duration_seconds%20%3E%2045%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-24h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "summary": "Reconciliation is failing or delayed in the {{ $labels.namespace_extracted }} stack"
            },
            "labels": {
              "app": "grafana-slo-operator",
              "severity": "low",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:01:44.284063693Z",
            "evaluationTime": 0.881327164
          },
          {
            "state": "inactive",
            "name": "SLO MTO Cert failures",
            "query": "sum by (cluster,namespace_extracted,slug)(count_over_time({namespace=\"grafana-slo-operator\", debug=\"\", container=\"grafana-slo-operator\"} | json | logger=\"grafana-slo-operator\" | _level=\"error\" |= \"x509\"[2h]))",
            "duration": 14400,
            "annotations": {
              "description": "MTO in {{ $labels.cluster }} is failing to verify its certificates to communicate with the app platform for at least 4 hours. Try restarting any pods as a first step.",
              "summary": "{{ $labels.cluster }}: MTO failing to verify certificates"
            },
            "labels": {
              "app": "grafana-slo-operator",
              "severity": "warn",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:01:45.16540024Z",
            "evaluationTime": 0.749358071
          }
        ],
        "totals": null,
        "interval": 300,
        "lastEvaluation": "2024-10-12T18:01:41.651481378Z",
        "evaluationTime": 4.263281657
      },
      {
        "name": "VPAQuickOOMLoop",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "VPAQuickOOMLoop",
            "query": "sum by (cluster,namespace)(count_over_time({namespace=\"vpa\"} |= \"quick OOM\" | pattern \"\u003c_\u003e quick OOM detected in pod \u003cnamespace\u003e/\u003cpod\u003e, container \u003ccontainer\u003e\" | label_format namespace=namespace_extracted,pod=pod_extracted,container=container_extracted[15m]))",
            "duration": 900,
            "annotations": {
              "message": "VPA detected a quick OOM loop in the {{ $labels.namespace }} namespace of the {{ $labels.cluster }} cluster for the {{ $labels.container }} container. This log indicates that VPA may be unable to increase the resources for a workload which is experiencing repeated OOM errors.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/vpa.md#VPAQuickOOMLoop"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:30.013566132Z",
            "evaluationTime": 1.027980094
          },
          {
            "state": "inactive",
            "name": "VPAQuickOOMLoop",
            "query": "sum by (cluster,namespace)(count_over_time({namespace=\"vpa\"} |= \"quick OOM\" | pattern \"\u003c_\u003e quick OOM detected in pod \u003cnamespace\u003e/\u003cpod\u003e, container \u003ccontainer\u003e\" | label_format namespace=namespace_extracted,pod=pod_extracted,container=container_extracted[15m]))",
            "duration": 3600,
            "annotations": {
              "message": "VPA detected a quick OOM loop in the {{ $labels.namespace }} namespace of the {{ $labels.cluster }} cluster for the {{ $labels.container }} container. This log indicates that VPA may be unable to increase the resources for a workload which is experiencing repeated OOM errors.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/vpa.md#VPAQuickOOMLoop"
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:31.041557579Z",
            "evaluationTime": 1.11843899
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:30.013528981Z",
        "evaluationTime": 2.146472818
      },
      {
        "name": "infra-recording-rules",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "kernel:device_remounted:count10m",
            "query": "(count_over_time({syslog_identifier=\"kernel\"} |= \"Remounting\" | pattern \"\u003c_\u003e (\u003cdevice\u003e)\" | label_format nodename=\"{{ .nodename | lower }}\"[10m]) \u003e 0)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:59.754081697Z",
            "evaluationTime": 1.358992916
          },
          {
            "name": "kernel:journal_aborted:count10m",
            "query": "(count_over_time({syslog_identifier=\"kernel\"} |= \"Aborting journal on device\" | pattern \"Aborting journal on device \u003cdevice\u003e-\u003c_\u003e\" | label_format nodename=\"{{ .nodename | lower }}\"[10m]) \u003e 0)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:01.113084035Z",
            "evaluationTime": 1.839052742
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:59.75406177Z",
        "evaluationTime": 3.198081178
      },
      {
        "name": "gateway",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "IncreasedIstioErrorRate",
            "query": "(sum by (cluster)(rate({name=\"hosted-grafana-gateway\", container=\"istio-proxy\"} |= \".hosted-grafana.svc.cluster.local\" | json | response_code!=\"200\" | response_flags=\"UF\" | __error__=\"\"[1m])) \u003e 10)",
            "duration": 600,
            "annotations": {
              "dashboard_url": "https://ops.grafana-ops.net/d/bnG7xxjGk/hosted-grafana-gateway",
              "message": "The error rate for istio is elevated.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/hosted-grafana/runbooks.md#IncreasedIstioErrorRate"
            },
            "labels": {
              "namespace": "hosted-grafana",
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:43.976779966Z",
            "evaluationTime": 0.796721389
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:43.976756377Z",
        "evaluationTime": 0.79675081
      },
      {
        "name": "grafana_alerting_remote_alertmanager",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "firing",
            "name": "RemotePrimaryAlertmanagerNonDefaultConfigurations",
            "query": "(sum by (cluster,user)(rate({cluster=~\".*\", job=\"alertmanager/alertmanager\"} | logfmt | msg=\"merging configurations not implemented, using mimir config\"[1m])) \u003e 0)",
            "duration": 600,
            "annotations": {
              "message": "Cloud Alertmanager tenant {{ $labels.user }} on cluster {{ $labels.cluster }} has non-default configurations for both Alertmanagers.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/hosted-grafana/runbooks.md#RemotePrimaryAlertmanagerNonDefaultConfigurations"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "RemotePrimaryAlertmanagerNonDefaultConfigurations",
                  "cluster": "ops-us-east-0",
                  "severity": "warning",
                  "team": "alerting",
                  "user": "106233"
                },
                "annotations": {
                  "message": "Cloud Alertmanager tenant 106233 on cluster ops-us-east-0 has non-default configurations for both Alertmanagers.\n",
                  "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/hosted-grafana/runbooks.md#RemotePrimaryAlertmanagerNonDefaultConfigurations"
                },
                "state": "firing",
                "activeAt": "2024-10-11T20:03:09.558187607Z",
                "value": "1.6666666666666666e-01"
              }
            ],
            "labels": {
              "severity": "warning",
              "team": "alerting"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:09.559287362Z",
            "evaluationTime": 2.753200622
          },
          {
            "state": "inactive",
            "name": "RemoteSecondaryAlertmanagerStateSyncErrors",
            "query": "(sum by (cluster,slug)(rate({job=\"hosted-grafana/grafana\", container=\"grafana\"} | logfmt | logger=\"ngalert.forked-alertmanager.remote-secondary\" | msg=\"Unable to upload the state to the remote Alertmanager\"[1m])) \u003e 0)",
            "duration": 900,
            "annotations": {
              "message": "instance {{ $labels.slug }} on cluster {{ $labels.cluster }} is failing to upload the Grafana Alertmanager state to the remote Alertmanager.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/hosted-grafana/runbooks.md#RemoteSecondaryAlertmanagerStateSyncErrors"
            },
            "labels": {
              "severity": "warning",
              "team": "alerting"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:12.312494369Z",
            "evaluationTime": 0.821663026
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:09.559276298Z",
        "evaluationTime": 3.574885208
      },
      {
        "name": "KarpenterUnfulfillableCapacity",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "KarpenterUnfulfillableCapacity",
            "query": "sum by (cluster,nodepool)(label_replace(count_over_time({namespace=\"karpenter\", job=\"karpenter/karpenter\"} |~ \"(?i)Unable to fulfill capacity due to your request configuration. Please adjust your request and try again\" | json | level=\"ERROR\" | NodeClaim_name!~\".*spot.*\"[10m]),\"nodepool\",\"$1\",\"NodeClaim_name\",\"(.*)-.*\"))",
            "duration": 600,
            "annotations": {
              "logs_url": "https://admin-ops-eu-south-0.grafana-ops.net/grafana/explore?schemaVersion=1\u0026panes=%7B%22sbk%22:%7B%22datasource%22:%22loki-ops%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22sum%20by%20%28cluster,%20nodepool%29%5Cn%28%5Cn%20%20%20%20label_replace%28%5Cn%20%20%20%20%20%20%20%20rate%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%7Bnamespace%3D%5C%22karpenter%5C%22,%20job%3D%5C%22karpenter%2Fkarpenter%5C%22%7D%20%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%7C~%20%60%28%3Fi%29Unable%20to%20fulfill%20capacity%20due%20to%20your%20request%20configuration.%20Please%20adjust%20your%20request%20and%20try%20again%60%20%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%7C%20json%20%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%7C%20level%20%3D%20%60ERROR%60%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%7C%20NodeClaim_name%20%21~%20%60.%2Aspot.%2A%60%5Cn%20%20%20%20%20%20%20%20%20%20%20%20%5B10m%5D%5Cn%20%20%20%20%20%20%20%20%29,%5Cn%20%20%20%20%20%20%20%20%5C%22nodepool%5C%22,%20%5C%22$1%5C%22,%20%5C%22NodeClaim_name%5C%22,%20%5C%22%28.%2A%29-.%2A%5C%22%5Cn%20%20%20%20%29%5Cn%29%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22loki-ops%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-30m%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "message": "AWS capacity exhausted for nodepool {{ $labels.nodepool }} on cluster {{ $labels.cluster }}",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/karpenter.md#KarpenterUnfulfillableCapacity"
            },
            "labels": {
              "severity": "warning",
              "team": "capacity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:13.911714955Z",
            "evaluationTime": 1.365555537
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:13.911693651Z",
        "evaluationTime": 1.365581443
      },
      {
        "name": "grafana",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "GrafanaInstancesFailingToLookupDNS",
            "query": "(sum by (cluster,namespace)(rate({job=\"hosted-grafana/grafana\", container=\"grafana\"} |~ \"dial tcp: lookup .*-hosted-grafana-\" | logfmt | __error__=\"\"[1m])) \u003e 1)",
            "duration": 900,
            "annotations": {
              "message": "grafana instances in {{ $labels.cluster }} are having problems resolving DNS.\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/hosted-grafana/runbooks.md#GrafanaInstancesFailingToLookupDNS"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:31.933197023Z",
            "evaluationTime": 1.16790609
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:31.933172337Z",
        "evaluationTime": 1.16793514
      },
      {
        "name": "grafana-slo-app-alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "SLOPluginPanic",
            "query": "sum by (cluster,slug)(rate({namespace=\"hosted-grafana\", debug=\"\", cluster=~\".+\", slug=~\".+\"} |= \"plugin process exited\" |= \"exit status 2\" != \"logger=tsdb.loki\" |= \"logger=plugin.grafana-slo-app\"[1h]))",
            "annotations": {
              "description": "Detecting a panic in the {{ $labels.slug }} stack.",
              "explore": "https://ops.grafana-ops.net/goto/3y0My4rSR?orgId=1",
              "summary": "SLO Plugin backends are panicking."
            },
            "labels": {
              "app": "grafana-slo-app",
              "severity": "low",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T17:59:00.186931125Z",
            "evaluationTime": 6.304132649
          }
        ],
        "totals": null,
        "interval": 300,
        "lastEvaluation": "2024-10-12T17:59:00.186896865Z",
        "evaluationTime": 6.304172284
      },
      {
        "name": "loki_cell_throughput",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_cell:throughput:successful_requests:rate1m",
            "query": "sum by (cluster,namespace,container,query_type,sharded)(rate({job=~\"loki.*/.*query-frontend\"} |= \"metrics.go\" !~ \"org_id=(123838|271882)\" | logfmt | source!=\"logvolhist\" | query_type=~\"filter|limited|metric\" | label_format sharded=\"{{ if (gt .shards .splits ) }}true{{else}}false{{end}}\" | ( duration\u003c10s or throughput\u003e=4.0GB )[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:46.226398915Z",
            "evaluationTime": 2.65259137
          },
          {
            "name": "loki_cell:throughput:total_requests:rate1m",
            "query": "sum by (cluster,namespace,container,query_type,sharded)(rate({job=~\"loki.*/.*query-frontend\"} |= \"metrics.go\" !~ \"org_id=(123838|271882)\" | logfmt | source!=\"logvolhist\" | query_type=~\"filter|limited|metric\" | label_format sharded=\"{{ if (gt .shards .splits ) }}true{{else}}false{{end}}\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:48.878998485Z",
            "evaluationTime": 1.3650451590000001
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:46.226374338Z",
        "evaluationTime": 4.017675281
      },
      {
        "name": "grafana-slo-app-requests",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "slo_plugin:synthetic_requests:rate5m",
            "query": "sum by (slug,cluster,status)(rate({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\"sloappmeta.+\"} |~ \"path=/api/plugins/grafana-slo-app/resources.*\" |~ \"status=[1-5][0-9][0-9]\" | pattern \"\u003c_\u003e status=\u003cstatus\u003e \u003c_\u003e\"[5m]))",
            "labels": {
              "app": "grafana-slo-app",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:40.657414882Z",
            "evaluationTime": 1.422917849
          },
          {
            "name": "slo_plugin:requests:rate5m",
            "query": "sum by (slug,cluster,status)(rate({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug!~\"sloappmeta.+\"} |~ \"path=/api/plugins/grafana-slo-app/resources.*\" |~ \"status=[1-5][0-9][0-9]\" | pattern \"\u003c_\u003e status=\u003cstatus\u003e \u003c_\u003e\"[5m]))",
            "labels": {
              "app": "grafana-slo-app",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:42.080346437Z",
            "evaluationTime": 1.3648449
          },
          {
            "name": "slo_plugin:crud_requests:rate5m",
            "query": "sum by (cluster,slug,Domain,Event,Provider,AlertRules)(rate({namespace=\"hosted-grafana\", debug=\"\", container=\"grafana\", cluster=~\".+\", slug=~\".+\"} | logfmt | ( logger=\"plugin.grafana-slo-app\" , msg=\"Event Log:\" )[5m]))",
            "labels": {
              "app": "grafana-slo-app",
              "team": "gops_slos"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:43.445200859Z",
            "evaluationTime": 3.03680521
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:40.657404556Z",
        "evaluationTime": 5.824607881
      },
      {
        "name": "gcp-kms-audit-alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "GCPKMSKeyScheduledForDestruction",
            "query": "((sum by (resource_name,actor_email)(count_over_time({job=\"cloud/gcp\", resource_type=\"cloudkms_cryptokeyversion\"} | json resource_name=\"protoPayload.resourceName\",method=\"protoPayload.methodName\",actor_email=\"protoPayload.authenticationInfo.principalEmail\" | method=\"DestroyCryptoKeyVersion\"[1w])) - on (resource_name) group_right (actor_email) sum by (resource_name)(count_over_time({job=\"cloud/gcp\", resource_type=\"cloudkms_cryptokeyversion\"} | json resource_name=\"protoPayload.resourceName\",method=\"protoPayload.methodName\",actor_email=\"protoPayload.authenticationInfo.principalEmail\" | method=\"RestoreCryptoKeyVersion\"[1w]))) \u003e 0)",
            "annotations": {
              "description": "A Google Cloud KMS key version has been scheduled for deletion.\nThis will lead to data loss if the key is still in use. Please audit this change, contacting the actor as appropriate.\nAfter 12h, if the key is not recovered or this alert silenced, the oncaller will be paged.\n\nActor email: {{ $labels.actor_email }}\nKey name: {{ $labels.resource_name }}\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/runbooks/gcp.md#GCPKMSKeyScheduledForDestruction",
              "summary": "GCP KMS scheduled for deletion"
            },
            "labels": {
              "severity": "warning"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:24.216164046Z",
            "evaluationTime": 1.436668365
          },
          {
            "state": "inactive",
            "name": "GCPKMSKeyScheduledForDestruction",
            "query": "((sum by (resource_name,actor_email)(count_over_time({job=\"cloud/gcp\", resource_type=\"cloudkms_cryptokeyversion\"} | json resource_name=\"protoPayload.resourceName\",method=\"protoPayload.methodName\",actor_email=\"protoPayload.authenticationInfo.principalEmail\" | method=\"DestroyCryptoKeyVersion\"[1w])) - on (resource_name) group_right (actor_email) sum by (resource_name)(count_over_time({job=\"cloud/gcp\", resource_type=\"cloudkms_cryptokeyversion\"} | json resource_name=\"protoPayload.resourceName\",method=\"protoPayload.methodName\",actor_email=\"protoPayload.authenticationInfo.principalEmail\" | method=\"RestoreCryptoKeyVersion\"[1w]))) \u003e 0)",
            "duration": 43200,
            "annotations": {
              "description": "A Google Cloud KMS key version has been scheduled for deletion.\nThis will lead to data loss if the key is still in use. Please audit this change, contacting the actor as appropriate.\nAfter 12h, if the key is not recovered or this alert silenced, the oncaller will be paged.\n\nActor email: {{ $labels.actor_email }}\nKey name: {{ $labels.resource_name }}\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/platform/runbooks/gcp.md#GCPKMSKeyScheduledForDestruction",
              "summary": "GCP KMS scheduled for deletion"
            },
            "labels": {
              "severity": "critical"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:25.652841208Z",
            "evaluationTime": 1.584483245
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:24.216131163Z",
        "evaluationTime": 3.021198395
      },
      {
        "name": "HostedGrafanaEdge-prod-us-east-0",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:http_requests:rate5m",
            "query": "sum by (cluster,slug,response_code,response_flags,exclude_from_sla)(rate({name=\"hosted-grafana-gateway\", container=\"istio-proxy\", cluster=\"prod-us-east-0\"} |= \"-grafana-http.hosted-grafana.svc.cluster.local\" | regexp \"(?P\u003cslug\u003e\\\\w+)-grafana-http.hosted-grafana.svc.cluster.local\" | json | label_format exclude_from_sla=\"{{if eq (regexReplaceAll \\\"/apis/.*|/api/live/ws|/api/datasources/proxy/.*|/api/ds/query|/api/tsdb/query|/api/datasources/.+/health|/api/datasources/.+/resources.*|/api/datasources/uid/.+/resources/*|/api/plugins/.+/resources.*|/api/plugin-proxy/.*|/api/.+/rules|/api/prometheus/.+/api/v1/rules|/api/ruler/.+/api/v1/rules\\\" .path \\\"exclude_from_sla\\\")  \\\"exclude_from_sla\\\"}}true{{else}}false{{end}}\"[5m]))",
            "labels": {
              "namespace": "hosted-grafana"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T17:59:10.077891312Z",
            "evaluationTime": 5.189679322
          }
        ],
        "totals": null,
        "interval": 240,
        "lastEvaluation": "2024-10-12T17:59:10.077868257Z",
        "evaluationTime": 5.189709076
      },
      {
        "name": "grafana-k8s-app",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "grafana_k8s_app:requests:rate5m",
            "query": "sum by (cluster,slug,stackId)(rate({app=\"grafana\", container=\"grafana\", cluster!=\"\", slug!=\"\", stackId!=\"\", debug=\"\", org!=\"ops\", conprof=\"true\", job=\"hosted-grafana/grafana\", name=\"grafana\", namespace=\"hosted-grafana\", stream=\"stdout\"} |= \"level=info\" |= \"path=/api/ds/query\" |~ \"referer=\\\".+/a/grafana-k8s-app.*\\\"\"[5m]))",
            "labels": {
              "app": "grafana-k8s-app"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:13.45964808Z",
            "evaluationTime": 2.077148811
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:13.459636477Z",
        "evaluationTime": 2.077167264
      },
      {
        "name": "HostedGrafanaDataSourcePluginLogs",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:grafana_datasource_querydata_logs:rate1m",
            "query": "sum by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs,statusSource)(rate({app=\"grafana\", namespace=\"hosted-grafana\"} | logfmt | __error__=\"\" | logger=\"plugin.instrumentation\" | endpoint=\"queryData\" | label_format fromAlert=\"{{if eq .uname \\\"grafana_scheduler\\\"}}true{{else}}false{{end}}\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:51.175735788Z",
            "evaluationTime": 2.8608251449999997
          },
          {
            "name": "hosted_grafana:grafana_loki_elasticsearch_response:rate1m",
            "query": "sum by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs)(rate({app=\"grafana\", container=\"grafana\", namespace=\"hosted-grafana\"} | logfmt | __error__=\"\" | endpoint=\"queryData\" | pluginId=~\"loki|elasticsearch\" | stage=\"parseResponse\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\" | label_format status=\"{{if eq \\\"stderr\\\" .stream}}error{{else}}ok{{end}}\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:54.036571731Z",
            "evaluationTime": 1.427100025
          },
          {
            "name": "hosted_grafana:grafana_loki_elasticsearch_request:rate1m",
            "query": "sum by (status,pluginId,statusCode,fromAlert,isGrafanaCloudDs,statusSource,cluster)(rate({app=\"grafana\", container=\"grafana\", namespace=\"hosted-grafana\"} | logfmt | pluginId=~\"loki|elasticsearch\" | endpoint=\"queryData\" | stage=\"databaseRequest\" | statusCode!=\"\" | label_format fromAlert=\"{{if eq .uname \\\"grafana_scheduler\\\"}}true{{else}}false{{end}}\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:55.463685409Z",
            "evaluationTime": 2.245710249
          },
          {
            "name": "hosted_grafana:grafana_datasource_querydata_latency_p99:rate1m",
            "query": "quantile_over_time(0.99,{app=\"grafana\", namespace=\"hosted-grafana\"} |= \"duration=\" | logfmt | __error__=\"\" | endpoint=\"queryData\" | logger=\"plugin.instrumentation\" | msg=\"Plugin Request Completed\" | label_format fromAlert=\"{{if eq .uname \\\"grafana_scheduler\\\"}}true{{else}}false{{end}}\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\" | unwrap duration(duration)[1m]) by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:57.709409003Z",
            "evaluationTime": 2.278455635
          },
          {
            "name": "hosted_grafana:grafana_datasource_querydata_latency_p90:rate1m",
            "query": "quantile_over_time(0.9,{app=\"grafana\", namespace=\"hosted-grafana\"} |= \"duration=\" | logfmt | __error__=\"\" | endpoint=\"queryData\" | logger=\"plugin.instrumentation\" | msg=\"Plugin Request Completed\" | label_format fromAlert=\"{{if eq .uname \\\"grafana_scheduler\\\"}}true{{else}}false{{end}}\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\" | unwrap duration(duration)[1m]) by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:59.987877666Z",
            "evaluationTime": 2.628899083
          },
          {
            "name": "hosted_grafana:grafana_datasource_querydata_latency_p50:rate1m",
            "query": "quantile_over_time(0.5,{app=\"grafana\", namespace=\"hosted-grafana\"} |= \"duration=\" | logfmt | __error__=\"\" | endpoint=\"queryData\" | logger=\"plugin.instrumentation\" | msg=\"Plugin Request Completed\" | label_format fromAlert=\"{{if eq .uname \\\"grafana_scheduler\\\"}}true{{else}}false{{end}}\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\" | unwrap duration(duration)[1m]) by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:02.616789737Z",
            "evaluationTime": 2.592954617
          },
          {
            "name": "hosted_grafana:grafana_loki_elasticsearch_response_latency_p99:rate1m",
            "query": "quantile_over_time(0.99,{app=\"grafana\", namespace=\"hosted-grafana\", container=\"grafana\"} |= \"duration=\" | logfmt | __error__=\"\" | pluginId=~\"loki|elasticsearch\" | stage=\"parseResponse\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\" | unwrap duration(duration)[1m]) by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:05.209756088Z",
            "evaluationTime": 1.296679822
          },
          {
            "name": "hosted_grafana:grafana_loki_elasticsearch_response_latency_p90:rate1m",
            "query": "quantile_over_time(0.9,{app=\"grafana\", namespace=\"hosted-grafana\", container=\"grafana\"} |= \"duration=\" | logfmt | __error__=\"\" | pluginId=~\"loki|elasticsearch\" | stage=\"parseResponse\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\" | unwrap duration(duration)[1m]) by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:06.506447805Z",
            "evaluationTime": 1.186686434
          },
          {
            "name": "hosted_grafana:grafana_loki_elasticsearch_response_latency_p50:rate1m",
            "query": "quantile_over_time(0.5,{app=\"grafana\", namespace=\"hosted-grafana\", container=\"grafana\"} |= \"duration=\" | logfmt | __error__=\"\" | pluginId=~\"loki|elasticsearch\" | stage=\"parseResponse\" | label_format isGrafanaCloudDs=\"{{if hasPrefix \\\"grafanacloud\\\" .dsUID}}true{{else}}false{{end}}\" | unwrap duration(duration)[1m]) by (status,pluginId,cluster,fromAlert,isGrafanaCloudDs)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:06.344630028Z",
            "evaluationTime": 2.672748125
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:01:51.176537956Z",
        "evaluationTime": 17.840845964
      },
      {
        "name": "GitHub Runners",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "RunnerInstanceCreateFailure",
            "query": "(sum by (environment,error_name)(count_over_time({job=\"cloud/aws\", aws_log_group=~\"/aws/lambda/gh-runners.*(scale-up|pool)\"} | json | level=\"ERROR\" | message=\"Failed to create instance, create fleet failed.\"[1m])) \u003e 0)",
            "duration": 300,
            "annotations": {
              "logs_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22fns%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bjob%3D%5C%22cloud%2Faws%5C%22,%20aws_log_group%3D~%5C%22%2Faws%2Flambda%2Fgh-runners.%2A%28scale-up%7Cpool%29%5C%22%7D%20%7C%20json%20%7C%20level%3D%5C%22ERROR%5C%22%20%7C%20message%3D%5C%22Failed%20to%20create%20instance,%20create%20fleet%20failed.%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "message": "{{ $labels.error_name }}: Failed to create {{ $labels.environment }} instance"
            },
            "labels": {
              "severity": "info",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:03:05.582710112Z",
            "evaluationTime": 0.222033442
          },
          {
            "state": "inactive",
            "name": "AutoScalingEventFailure",
            "query": "(sum by (environment,service,error_name,error_message)(count_over_time({job=\"cloud/aws\", aws_log_group=~\"/aws/lambda/gh-runners.*(scale-up|scale-down)\"} | json | level=\"ERROR\" | message=~`Request failed with status code 5[0-9][0-9]`[1m])) \u003e 0)",
            "duration": 300,
            "annotations": {
              "logs_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22fns%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bjob%3D%5C%22cloud%2Faws%5C%22,%20aws_log_group%3D~%5C%22%2Faws%2Flambda%2Fgh-runners.%2A%28scale-up%7Cscale-down%29%5C%22%7D%20%7C%20json%20%7C%20level%3D%5C%22ERROR%5C%22%20%7C%20message%3D~%5C%22Request%20failed%20with%20status%20code%205%5B0-9%5D%5B0-9%5D%5C%22%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "message": "{{ $labels.error_name }}: {{ $labels.service }} {{ $labels.error_message }} for {{ $labels.environment }}"
            },
            "labels": {
              "severity": "info",
              "team": "productivity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:03:05.804754393Z",
            "evaluationTime": 0.514122075
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:03:05.582688091Z",
        "evaluationTime": 0.73619365
      },
      {
        "name": "HostedGrafanaEdge-smallclusters",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:http_requests:rate5m",
            "query": "sum by (cluster,slug,response_code,response_flags,exclude_from_sla)(rate({name=\"hosted-grafana-gateway\", container=\"istio-proxy\", cluster!=\"prod-us-east-0\", cluster!=\"prod-us-central-0\", cluster!=\"prod-eu-west-0\", cluster!=\"prod-eu-west-2\"} |= \"-grafana-http.hosted-grafana.svc.cluster.local\" | regexp \"(?P\u003cslug\u003e\\\\w+)-grafana-http.hosted-grafana.svc.cluster.local\" | json | label_format exclude_from_sla=\"{{if eq (regexReplaceAll \\\"/apis/.*|/api/live/ws|/api/datasources/proxy/.*|/api/ds/query|/api/tsdb/query|/api/datasources/.+/health|/api/datasources/.+/resources.*|/api/datasources/uid/.+/resources/*|/api/plugins/.+/resources.*|/api/plugin-proxy/.*|/api/.+/rules|/api/prometheus/.+/api/v1/rules|/api/ruler/.+/api/v1/rules\\\" .path \\\"exclude_from_sla\\\")  \\\"exclude_from_sla\\\"}}true{{else}}false{{end}}\"[5m]))",
            "labels": {
              "namespace": "hosted-grafana"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T17:59:26.062097064Z",
            "evaluationTime": 5.343839498
          }
        ],
        "totals": null,
        "interval": 240,
        "lastEvaluation": "2024-10-12T17:59:26.062082179Z",
        "evaluationTime": 5.343861131
      },
      {
        "name": "HostedGrafanaLogs",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:grafana_logs:rate1m",
            "query": "(sum by (level,cluster,namespace)(rate({job=\"hosted-grafana/grafana\", container=\"grafana\"} | logfmt | __error__=\"\"[1m])) or sum by (lvl,cluster,namespace)(rate({job=\"hosted-grafana/grafana\", container=\"grafana\"} | logfmt | lvl!=\"\" | __error__=\"\"[1m])))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:30.206276273Z",
            "evaluationTime": 2.53762787
          },
          {
            "name": "hosted_grafana:grafana_frontend_logs:rate1m",
            "query": "(sum by (level,cluster,namespace)(rate({job=\"hosted-grafana/grafana\", container=\"grafana\"} | logfmt | logger=\"frontend\" | __error__=\"\"[1m])) or sum by (lvl,cluster,namespace)(rate({job=\"hosted-grafana/grafana\", container=\"grafana\"} | logfmt | logger=\"frontend\" | lvl!=\"\" | __error__=\"\"[1m])))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:32.743915274Z",
            "evaluationTime": 2.714024908
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:30.206254387Z",
        "evaluationTime": 5.251689942
      },
      {
        "name": "asserts-loki-alerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "AssertsGCOMErrors",
            "query": "sum by (cluster,namespace,job,tenantId)(rate({namespace=\"asserts\", level=\"ERROR\"} |= \"GComClient\" | logfmt[5m]))",
            "duration": 300,
            "annotations": {
              "message": "GCOM Errors for Tenant={{ $labels.tenantId }} in Cluster={{ $labels.cluster }}"
            },
            "labels": {
              "loki_alert": "true",
              "severity": "warning",
              "team": "asserts"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:52.980030766Z",
            "evaluationTime": 0.812400884
          },
          {
            "state": "inactive",
            "name": "AssertsModelBuildingErrors",
            "query": "sum by (tenant,cluster,job)(rate({namespace=\"asserts\", level=\"ERROR\", job=\"asserts/model-builder\"} |= \"Failed to process property rule\" | logfmt[5m]))",
            "duration": 300,
            "annotations": {
              "message": "PromQL Errors for Tenant={{ $labels.tenant }} in Cluster={{ $labels.cluster }} \u003chttps://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22ioo%22:%7B%22datasource%22:%22000000193%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bname%3D%5C%22model-builder%5C%22,%20level%3D%5C%22ERROR%5C%22,%20cluster%3D%5C%22{{ $labels.cluster }}%5C%22%7D%20%7C%3D%20%60PropertyRuleProcessor%60%20%7C~%20%60EntityProcessor%5C%5Cs%2BtenantId%3D{{ $labels.tenant }}%5C%5Cs%2BFailed%20to%20process%20property%20rule%60%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22000000193%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1|See Logs\u003e"
            },
            "labels": {
              "loki_alert": "true",
              "severity": "warning",
              "team": "asserts"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:53.792440859Z",
            "evaluationTime": 1.741890903
          },
          {
            "state": "inactive",
            "name": "AssertsGraphGComEntryMissing",
            "query": "sum by (tenantId)(rate({namespace=\"asserts\", level=\"ERROR\", job=\"asserts/model-builder\"} |~ \"assertsGraphInstance for stackId=.* not found in GCom\" | logfmt[5m]))",
            "duration": 300,
            "annotations": {
              "message": "asserts-graph-instances GCom record missing for Tenant={{ $labels.tenantId }}"
            },
            "labels": {
              "loki_alert": "true",
              "severity": "warning",
              "team": "asserts"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:55.534341245Z",
            "evaluationTime": 0.542544552
          },
          {
            "state": "inactive",
            "name": "AssertsMimirPublishErrors",
            "query": "sum by (cluster,namespace,job,tenantId)(rate({namespace=\"asserts\", level=\"ERROR\"} |= \"MimirRulesPublisher\" | logfmt[5m]))",
            "duration": 300,
            "annotations": {
              "message": "MimirPublish Errors for Tenant={{ $labels.tenantId }} in Cluster={{ $labels.cluster }}"
            },
            "labels": {
              "loki_alert": "true",
              "severity": "warning",
              "team": "asserts"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:56.076895043Z",
            "evaluationTime": 1.524550252
          },
          {
            "state": "inactive",
            "name": "AssertsMimirRelabelPublishErrors",
            "query": "sum by (cluster,namespace,job,tenantId)(rate({namespace=\"asserts\", level=\"ERROR\"} |= \"MimirRelabelRulesPublisher\" | logfmt[5m]))",
            "duration": 300,
            "annotations": {
              "message": "MimirRelabelPublish Errors for Tenant={{ $labels.tenantId }} in Cluster={{ $labels.cluster }}"
            },
            "labels": {
              "loki_alert": "true",
              "severity": "warning",
              "team": "asserts"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:57.601454373Z",
            "evaluationTime": 1.107327484
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:52.9800068Z",
        "evaluationTime": 5.728780103
      },
      {
        "name": "unused_metrics_response_too_old",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "UnusedMetricsResponseTooOld",
            "query": "((quantile_over_time(0.5,{namespace=\"usage-service\", job=\"usage-service/metrics-api\"} |= \"returned UsageAnalysis\" | json | unwrap age_seconds[1d]) by (cluster,namespace) \u003e 86400) and (count by (cluster,namespace)(max_over_time({namespace=\"usage-service\", job=\"usage-service/metrics-api\"} |= \"returned UsageAnalysis\" | json | unwrap age_seconds[1d]) by (instance,cluster,namespace)) \u003e 10))",
            "duration": 3600,
            "annotations": {
              "message": "The Unused metrics service on cluster {{ $labels.cluster }} is returning data that is too old\n",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/insights/usage_service_runbook.md#UnusedMetricsResponseTooOld"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:33.753667176Z",
            "evaluationTime": 2.823160343
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:33.753640418Z",
        "evaluationTime": 2.823191645
      },
      {
        "name": "CacheMetrics",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_cell:chunk_cache_hit_rate_no_canary:rate1m",
            "query": "avg by (cluster,namespace)(avg_over_time({job=~\".*/querier\"} |= \"metrics.go\" != \"/querier\" != \"loki-canary\" | logfmt | cache_chunk_req\u003e0 | label_format hit_ratio=\"{{ mulf (divf .cache_chunk_hit .cache_chunk_req) 100 }}\" | unwrap hit_ratio[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:38.58917801Z",
            "evaluationTime": 1.010964161
          },
          {
            "name": "loki_cell:chunk_cache_hit_rate_with_canary:rate1m",
            "query": "avg by (cluster,namespace)(avg_over_time({job=~\".*/querier\"} |= \"metrics.go\" != \"/querier\" | logfmt | cache_chunk_req\u003e0 | label_format hit_ratio=\"{{ mulf (divf .cache_chunk_hit .cache_chunk_req) 100 }}\" | unwrap hit_ratio[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:39.60015232Z",
            "evaluationTime": 2.414218127
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:38.589166384Z",
        "evaluationTime": 3.425209899
      },
      {
        "name": "mimir_query_stats_metrics",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "mimir_user:queries:rate1m",
            "query": "sum by (cluster,namespace,user,status,fromalert,attribution)(rate({job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | label_format attribution=\"\\n        {{- if hasPrefix \\\"adaptive-metrics/\\\" .user_agent }}adaptive-metrics\\n        {{- else if hasPrefix \\\"usage_service\\\" .user_agent }}usage-service\\n        {{- else if hasPrefix \\\"asserts-api-server\\\" .user_agent }}asserts-api-server\\n        {{- else if hasPrefix \\\"asserts-assertion-detector\\\" .user_agent }}asserts-assertion-detector\\n        {{- else if hasPrefix \\\"asserts-model-builder\\\" .user_agent }}asserts-model-builder\\n        {{- else if hasPrefix \\\"Template-miner\\\" .user_agent }}adaptive-logs\\n        {{- else if eq .header_fromalert \\\"true\\\" }}grafana-alerting\\n        {{- else}}unknown{{end -}}\\n        \" | __error__=\"\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:18.731840166Z",
            "evaluationTime": 4.516052971
          },
          {
            "name": "mimir_user:query_wall_time_seconds:rate1m",
            "query": "sum by (cluster,namespace,user,status,fromalert,attribution)(rate({job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | label_format attribution=\"\\n        {{- if hasPrefix \\\"adaptive-metrics/\\\" .user_agent }}adaptive-metrics\\n        {{- else if hasPrefix \\\"usage_service\\\" .user_agent }}usage-service\\n        {{- else if hasPrefix \\\"asserts-api-server\\\" .user_agent }}asserts-api-server\\n        {{- else if hasPrefix \\\"asserts-assertion-detector\\\" .user_agent }}asserts-assertion-detector\\n        {{- else if hasPrefix \\\"asserts-model-builder\\\" .user_agent }}asserts-model-builder\\n        {{- else if hasPrefix \\\"Template-miner\\\" .user_agent }}adaptive-logs\\n        {{- else if eq .header_fromalert \\\"true\\\" }}grafana-alerting\\n        {{- else}}unknown{{end -}}\\n        \" | unwrap query_wall_time_seconds | __error__=\"\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:23.247912074Z",
            "evaluationTime": 3.036531219
          },
          {
            "name": "mimir_user:query_fetched_chunk_bytes:rate1m",
            "query": "sum by (cluster,namespace,user,status,fromalert,attribution)(rate({job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | label_format attribution=\"\\n        {{- if hasPrefix \\\"adaptive-metrics/\\\" .user_agent }}adaptive-metrics\\n        {{- else if hasPrefix \\\"usage_service\\\" .user_agent }}usage-service\\n        {{- else if hasPrefix \\\"asserts-api-server\\\" .user_agent }}asserts-api-server\\n        {{- else if hasPrefix \\\"asserts-assertion-detector\\\" .user_agent }}asserts-assertion-detector\\n        {{- else if hasPrefix \\\"asserts-model-builder\\\" .user_agent }}asserts-model-builder\\n        {{- else if hasPrefix \\\"Template-miner\\\" .user_agent }}adaptive-logs\\n        {{- else if eq .header_fromalert \\\"true\\\" }}grafana-alerting\\n        {{- else}}unknown{{end -}}\\n        \" | unwrap fetched_chunk_bytes | __error__=\"\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:26.284459308Z",
            "evaluationTime": 3.697045136
          },
          {
            "name": "mimir_user:query_fetched_index_bytes:rate1m",
            "query": "sum by (cluster,namespace,user,status,fromalert,attribution)(rate({job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | label_format attribution=\"\\n        {{- if hasPrefix \\\"adaptive-metrics/\\\" .user_agent }}adaptive-metrics\\n        {{- else if hasPrefix \\\"usage_service\\\" .user_agent }}usage-service\\n        {{- else if hasPrefix \\\"asserts-api-server\\\" .user_agent }}asserts-api-server\\n        {{- else if hasPrefix \\\"asserts-assertion-detector\\\" .user_agent }}asserts-assertion-detector\\n        {{- else if hasPrefix \\\"asserts-model-builder\\\" .user_agent }}asserts-model-builder\\n        {{- else if hasPrefix \\\"Template-miner\\\" .user_agent }}adaptive-logs\\n        {{- else if eq .header_fromalert \\\"true\\\" }}grafana-alerting\\n        {{- else}}unknown{{end -}}\\n        \" | unwrap fetched_index_bytes | __error__=\"\"[1m]))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:29.981520796Z",
            "evaluationTime": 2.684336678
          },
          {
            "name": "mimir:query_frontend:query_context_cancel:ratio:rate1m",
            "query": "(sum by (cluster,namespace)(count_over_time({container=\"query-frontend\", namespace=~\"(mimir|cortex).*\"} |= \"query stats\" |= \"context canceled\" | logfmt | status!=\"success\"[1m])) / on (cluster,namespace)  sum by (cluster,namespace)(count_over_time({container=\"query-frontend\", namespace=~\"(mimir|cortex).*\"} |= \"query stats\"[1m])))",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:32.665873254Z",
            "evaluationTime": 2.13758183
          },
          {
            "name": "mimir_user:query_response_time_seconds_p99:rate1m",
            "query": "quantile_over_time(0.99,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap duration(response_time) | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:34.803465818Z",
            "evaluationTime": 2.022301189
          },
          {
            "name": "mimir_user:query_response_time_seconds_p90:rate1m",
            "query": "quantile_over_time(0.9,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap duration(response_time) | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:36.825780644Z",
            "evaluationTime": 2.625152741
          },
          {
            "name": "mimir_user:query_response_time_seconds_p50:rate1m",
            "query": "quantile_over_time(0.5,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap duration(response_time) | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:39.450949835Z",
            "evaluationTime": 2.288230602
          },
          {
            "name": "mimir_user:query_wall_time_seconds_p99:rate1m",
            "query": "quantile_over_time(0.99,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap query_wall_time_seconds | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:41.739196081Z",
            "evaluationTime": 1.594063707
          },
          {
            "name": "mimir_user:query_wall_time_seconds_p90:rate1m",
            "query": "quantile_over_time(0.9,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap query_wall_time_seconds | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:43.333272268Z",
            "evaluationTime": 3.657863072
          },
          {
            "name": "mimir_user:query_wall_time_seconds_p50:rate1m",
            "query": "quantile_over_time(0.5,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap query_wall_time_seconds | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:46.991152065Z",
            "evaluationTime": 2.474764589
          },
          {
            "name": "mimir_user:query_fetched_chunk_bytes_p99:rate1m",
            "query": "quantile_over_time(0.99,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap fetched_chunk_bytes | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:49.465933241Z",
            "evaluationTime": 2.023187575
          },
          {
            "name": "mimir_user:query_fetched_chunk_bytes_p90:rate1m",
            "query": "quantile_over_time(0.9,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap fetched_chunk_bytes | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:51.489136111Z",
            "evaluationTime": 2.070696289
          },
          {
            "name": "mimir_user:query_fetched_chunk_bytes_p50:rate1m",
            "query": "quantile_over_time(0.5,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap fetched_chunk_bytes | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:53.559847467Z",
            "evaluationTime": 2.471349375
          },
          {
            "name": "mimir_user:query_fetched_index_bytes_p99:rate1m",
            "query": "quantile_over_time(0.99,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap fetched_index_bytes | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:56.031209117Z",
            "evaluationTime": 2.40013685
          },
          {
            "name": "mimir_user:query_fetched_index_bytes_p90:rate1m",
            "query": "quantile_over_time(0.9,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap fetched_index_bytes | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:58.431358798Z",
            "evaluationTime": 2.362037201
          },
          {
            "name": "mimir_user:query_fetched_index_bytes_p50:rate1m",
            "query": "quantile_over_time(0.5,{job=~\".*/(query-frontend|mimir-read)\", namespace=~\"(mimir|cortex).*\"} |= \"msg=\\\"query stats\\\"\" | logfmt | label_format fromalert=\"{{ if ne .header_fromalert \\\"\\\" }}{{ .header_fromalert }}{{else}}false{{end}}\" | unwrap fetched_index_bytes | __error__=\"\"[1m]) by (cluster,namespace,user,fromalert)",
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:03:00.793410156Z",
            "evaluationTime": 3.153195199
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:18.731826731Z",
        "evaluationTime": 45.214785938
      },
      {
        "name": "HostedGrafanaEdge-prod-us-central-0",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:http_requests:rate5m",
            "query": "sum by (cluster,slug,response_code,response_flags,exclude_from_sla)(rate({name=\"hosted-grafana-gateway\", container=\"istio-proxy\", cluster=\"prod-us-central-0\"} |= \"-grafana-http.hosted-grafana.svc.cluster.local\" | regexp \"(?P\u003cslug\u003e\\\\w+)-grafana-http.hosted-grafana.svc.cluster.local\" | json | label_format exclude_from_sla=\"{{if eq (regexReplaceAll \\\"/apis/.*|/api/live/ws|/api/datasources/proxy/.*|/api/ds/query|/api/tsdb/query|/api/datasources/.+/health|/api/datasources/.+/resources.*|/api/datasources/uid/.+/resources/*|/api/plugins/.+/resources.*|/api/plugin-proxy/.*|/api/.+/rules|/api/prometheus/.+/api/v1/rules|/api/ruler/.+/api/v1/rules\\\" .path \\\"exclude_from_sla\\\")  \\\"exclude_from_sla\\\"}}true{{else}}false{{end}}\"[5m]))",
            "labels": {
              "namespace": "hosted-grafana"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T17:59:50.559608424Z",
            "evaluationTime": 5.105873839
          }
        ],
        "totals": null,
        "interval": 240,
        "lastEvaluation": "2024-10-12T17:59:50.559587932Z",
        "evaluationTime": 5.105900715
      },
      {
        "name": "GKEAlerts",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "GKENodePoolExhausted",
            "query": "sum by (cluster)(count_over_time({container=\"eventrouter\"} | logfmt | object_kind=\"ConfigMap\" | object_name=\"cluster-autoscaler-status\" | reason=\"ScaleUpFailed\" | message=~`.*OutOfResource.RESOURCE_POOL_EXHAUSTED.*` | message!~\".*-spot-.*\"[10m]))",
            "duration": 60,
            "annotations": {
              "description": "An on-demand nodepool in the {{ $labels.cluster }} cluster cannot scale up as needed due to lack of compute availability in that region.",
              "logs_url": "https://admin-ops-us-east-0.grafana-ops.net/grafana/explore?panes=%7B%22esI%22:%7B%22datasource%22:%22loki-ops%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bcluster%3D%5C%22{{ $labels.cluster }}%5C%22,container%3D%5C%22eventrouter%5C%22%7D%20%7C%20logfmt%20%7C%20object_kind%3D%5C%22ConfigMap%5C%22%20%7C%20object_name%3D%5C%22cluster-autoscaler-status%5C%22%20%7C%20reason%3D%5C%22ScaleUpFailed%5C%22%20%7C%20message%3D~%5C%22.%2AOutOfResource.RESOURCE_POOL_EXHAUSTED.%2A%5C%22%20%7C%20message%21~%5C%22.%2A-spot-.%2A%5C%22%20%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22loki-ops%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D%7D\u0026schemaVersion=1\u0026orgId=1",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/gcp.md#GKENodePoolExhausted",
              "summary": "GKE on-demand NodePool ScaleUp failure due to no availability"
            },
            "labels": {
              "severity": "warning",
              "team": "capacity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:03:01.895534034Z",
            "evaluationTime": 2.251922346
          },
          {
            "state": "inactive",
            "name": "GKENodePoolExhausted",
            "query": "sum by (cluster)(count_over_time({container=\"eventrouter\"} | logfmt | object_kind=\"ConfigMap\" | object_name=\"cluster-autoscaler-status\" | reason=\"ScaleUpFailed\" | message=~`.*OutOfResource.RESOURCE_POOL_EXHAUSTED.*` | message!~\".*-spot-.*\"[10m]))",
            "duration": 900,
            "annotations": {
              "description": "An on-demand nodepool in the {{ $labels.cluster }} cluster cannot scale up as needed due to lack of compute availability in that region.",
              "logs_url": "https://admin-ops-us-east-0.grafana-ops.net/grafana/explore?panes=%7B%22esI%22:%7B%22datasource%22:%22loki-ops%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bcluster%3D%5C%22{{ $labels.cluster }}%5C%22,container%3D%5C%22eventrouter%5C%22%7D%20%7C%20logfmt%20%7C%20object_kind%3D%5C%22ConfigMap%5C%22%20%7C%20object_name%3D%5C%22cluster-autoscaler-status%5C%22%20%7C%20reason%3D%5C%22ScaleUpFailed%5C%22%20%7C%20message%3D~%5C%22.%2AOutOfResource.RESOURCE_POOL_EXHAUSTED.%2A%5C%22%20%7C%20message%21~%5C%22.%2A-spot-.%2A%5C%22%20%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22loki-ops%22%7D,%22editorMode%22:%22code%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D%7D\u0026schemaVersion=1\u0026orgId=1",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/gcp.md#GKENodePoolExhausted",
              "summary": "GKE on-demand NodePool ScaleUp failure due to no availability"
            },
            "labels": {
              "severity": "critical",
              "team": "capacity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:03:04.147464877Z",
            "evaluationTime": 2.085294018
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:03:01.895504432Z",
        "evaluationTime": 4.337260094
      },
      {
        "name": "HostedGrafanaEdge-prod-eu-west-2",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "hosted_grafana:http_requests:rate5m",
            "query": "sum by (cluster,slug,response_code,response_flags,exclude_from_sla)(rate({name=\"hosted-grafana-gateway\", container=\"istio-proxy\", cluster=\"prod-eu-west-2\"} |= \"-grafana-http.hosted-grafana.svc.cluster.local\" | regexp \"(?P\u003cslug\u003e\\\\w+)-grafana-http.hosted-grafana.svc.cluster.local\" | json | label_format exclude_from_sla=\"{{if eq (regexReplaceAll \\\"/apis/.*|/api/live/ws|/api/datasources/proxy/.*|/api/ds/query|/api/tsdb/query|/api/datasources/.+/health|/api/datasources/.+/resources.*|/api/datasources/uid/.+/resources/*|/api/plugins/.+/resources.*|/api/plugin-proxy/.*|/api/.+/rules|/api/prometheus/.+/api/v1/rules|/api/ruler/.+/api/v1/rules\\\" .path \\\"exclude_from_sla\\\")  \\\"exclude_from_sla\\\"}}true{{else}}false{{end}}\"[5m]))",
            "labels": {
              "namespace": "hosted-grafana"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:01:15.53560416Z",
            "evaluationTime": 4.96981494
          }
        ],
        "totals": null,
        "interval": 240,
        "lastEvaluation": "2024-10-12T18:01:15.53558388Z",
        "evaluationTime": 4.969840684
      },
      {
        "name": "CellIngesterQueryThroughput:rate5m",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_cell:ingester_throughput:rate5m",
            "query": "sum by (cluster,namespace,container)(rate({job=~\"loki.*/.*querier\"} |= \"metrics.go\" | logfmt | unwrap bytes(ingester_chunk_decompressed_bytes)[5m]))",
            "labels": {
              "source": "memchunk"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:15.995172822Z",
            "evaluationTime": 2.900477725
          },
          {
            "name": "loki_cell:ingester_throughput:rate5m",
            "query": "sum by (cluster,namespace,container)(rate({job=~\"loki.*/.*querier\"} |= \"metrics.go\" | logfmt | unwrap bytes(ingester_chunk_head_bytes)[5m]))",
            "labels": {
              "source": "head"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:18.895661386Z",
            "evaluationTime": 1.9828233750000002
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:15.995160991Z",
        "evaluationTime": 4.883330427
      },
      {
        "name": "TenantMetricsLookback",
        "file": "ops-us-east-0",
        "rules": [
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | start_delta\u003c=3h0m0s[2m]))",
            "labels": {
              "period": "less than 3h"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:09.149372734Z",
            "evaluationTime": 2.382471018
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e3h0m0s , start_delta\u003c=12h0m0s )[2m]))",
            "labels": {
              "period": "between 3h and 12h"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:11.531857391Z",
            "evaluationTime": 1.953872823
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e12h0m0s , start_delta\u003c=24h0m0s )[2m]))",
            "labels": {
              "period": "between 12h and 1d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:13.485739004Z",
            "evaluationTime": 2.652276134
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e24h0m0s , start_delta\u003c=48h0m0s )[2m]))",
            "labels": {
              "period": "between 1d and 2d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:16.138025842Z",
            "evaluationTime": 0.630115482
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e48h0m0s , start_delta\u003c=72h0m0s )[2m]))",
            "labels": {
              "period": "between 2d and 3d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:16.768153794Z",
            "evaluationTime": 2.57213572
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e72h0m0s , start_delta\u003c=96h0m0s )[2m]))",
            "labels": {
              "period": "between 3d and 4d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:19.340300706Z",
            "evaluationTime": 2.077959596
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e96h0m0s , start_delta\u003c=120h0m0s )[2m]))",
            "labels": {
              "period": "between 4d and 5d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:21.418270936Z",
            "evaluationTime": 2.306083174
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e120h0m0s , start_delta\u003c=168h0m0s )[2m]))",
            "labels": {
              "period": "between 5d and 1w"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:23.724367344Z",
            "evaluationTime": 2.410750656
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e168h0m0s , start_delta\u003c=336h0m0s )[2m]))",
            "labels": {
              "period": "between 1w and 2w"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:26.135131678Z",
            "evaluationTime": 1.5262741420000001
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e336h0m0s , start_delta\u003c=720h0m0s )[2m]))",
            "labels": {
              "period": "between 2w and 1mo"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:27.66142051Z",
            "evaluationTime": 2.105011576
          },
          {
            "name": "loki_tenant:query_count:lookback_period",
            "query": "sum by (cluster,namespace,org_id)(count_over_time({container=\"query-frontend\", namespace=~\"loki.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | start_delta\u003e720h0m0s[2m]))",
            "labels": {
              "period": "over 1mo"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:29.766444097Z",
            "evaluationTime": 1.261369806
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | start_delta\u003c=3h0m0s | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "less than 3h"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:31.027824217Z",
            "evaluationTime": 3.198084147
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e3h0m0s , start_delta\u003c=12h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 3h and 12h"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:34.225920356Z",
            "evaluationTime": 2.6860875159999997
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e12h0m0s , start_delta\u003c=24h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 12h and 1d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:36.912018143Z",
            "evaluationTime": 2.811489145
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e24h0m0s , start_delta\u003c=48h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 1d and 2d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:39.723517002Z",
            "evaluationTime": 1.917410134
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e48h0m0s , start_delta\u003c=72h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 2d and 3d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:41.640939064Z",
            "evaluationTime": 1.9410028929999998
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e72h0m0s , start_delta\u003c=96h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 3d and 4d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:43.581949073Z",
            "evaluationTime": 1.492611409
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e96h0m0s , start_delta\u003c=120h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 4d and 5d"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:45.074570667Z",
            "evaluationTime": 2.073535557
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e120h0m0s , start_delta\u003c=168h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 5d and 1w"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:47.148117505Z",
            "evaluationTime": 3.904698289
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e168h0m0s , start_delta\u003c=336h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 1w and 2w"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:51.052827975Z",
            "evaluationTime": 3.7187307179999998
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | ( start_delta\u003e336h0m0s , start_delta\u003c=720h0m0s ) | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "between 2w and 1mo"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:54.771569917Z",
            "evaluationTime": 4.122831484
          },
          {
            "name": "loki_cell:processed_chunk_volume_bytes:lookback_period",
            "query": "sum by (cluster,namespace,container)(sum_over_time({namespace=~\"loki.*\", container=~\".*querier.*\"} |= \"caller=metrics.go\" != \"org_id=145265\" |= \"start_delta=\" | logfmt | start_delta\u003e720h0m0s | unwrap bytes(total_bytes) | __error__=\"\"[2m]))",
            "labels": {
              "period": "over 1mo"
            },
            "health": "ok",
            "type": "recording",
            "lastEvaluation": "2024-10-12T18:02:58.894413165Z",
            "evaluationTime": 3.4242560969999998
          }
        ],
        "totals": null,
        "interval": 120,
        "lastEvaluation": "2024-10-12T18:02:09.149356479Z",
        "evaluationTime": 53.169318281
      },
      {
        "name": "Cost Rule Evaluation",
        "file": "ops-us-east-0",
        "rules": [
          {
            "state": "inactive",
            "name": "CostRuleFailedToEvaluate",
            "query": "(sum by (group)(count_over_time({cluster=\"ops-us-east-0\", container=\"ruler\"} | logfmt | group=~\"cost_rules.*\" | msg=\"Evaluating rule failed\" | json | err!~\"rpc error: code = Code\\\\((500|429)\\\\).*\"[1m])) \u003e 0)",
            "duration": 300,
            "annotations": {
              "logs_url": "https://ops.grafana-ops.net/explore?schemaVersion=1\u0026panes=%7B%22d7w%22:%7B%22datasource%22:%22c-R8UWvVk%22,%22queries%22:%5B%7B%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22c-R8UWvVk%22%7D,%22editorMode%22:%22code%22,%22expr%22:%22sum%20by%20%28group%29%28%5Cn%20%20%20%20count_over_time%28%5Cn%20%20%20%20%20%20%20%20%7Bcluster%3D%5C%22ops-us-east-0%5C%22,%20container%3D%5C%22ruler%5C%22%7D%20%7C%20logfmt%20%7C%20group%3D~%5C%22cost_rules.%2A%5C%22%20%7C%20msg%3D%5C%22Evaluating%20rule%20failed%5C%22%5B1m%5D%5Cn%20%20%20%20%29%5Cn%29%20%3E%200%22,%22queryType%22:%22range%22,%22refId%22:%22A%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D%7D\u0026orgId=1",
              "message": "Mimir is failiing to evaluate the cost rule {{ $labels.group }}.",
              "runbook_url": "https://github.com/grafana/deployment_tools/blob/master/docs/capacity/runbooks/cost_rules.md#CostRuleFailedToEvaluate"
            },
            "labels": {
              "severity": "warning",
              "team": "capacity"
            },
            "health": "ok",
            "type": "alerting",
            "lastEvaluation": "2024-10-12T18:02:55.416898267Z",
            "evaluationTime": 2.036382562
          }
        ],
        "totals": null,
        "interval": 60,
        "lastEvaluation": "2024-10-12T18:02:55.416874131Z",
        "evaluationTime": 2.036412932
      }
    ]
  }
}
